{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the Required Libraries"
      ],
      "metadata": {
        "id": "dKW8RbAqUkxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from PIL import Image\n",
        "import io\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas"
      ],
      "metadata": {
        "id": "mS4p--lPTi7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "yYpIDYHjz5AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "BFZACkrM69YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(use_cuda)"
      ],
      "metadata": {
        "id": "OyhTHoR5z76x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Required Hyperparameters"
      ],
      "metadata": {
        "id": "F-s6vt_OUoRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HParams():\n",
        "    def __init__(self):\n",
        "        self.data_location = '/content/sketchrnn_apple.npz'\n",
        "        self.enc_hidden_size = 256\n",
        "        self.dec_hidden_size = 512\n",
        "        self.Nz = 128\n",
        "        self.M = 20\n",
        "        self.dropout = 0.9\n",
        "        self.batch_size = 100\n",
        "        self.eta_min = 0.01\n",
        "        self.R = 0.99995\n",
        "        self.KL_min = 0.2\n",
        "        self.wKL = 0.5\n",
        "        self.lr = 0.001\n",
        "        self.lr_decay = 0.9999\n",
        "        self.min_lr = 0.00001\n",
        "        self.grad_clip = 1.\n",
        "        self.temperature = 0.4\n",
        "        self.max_seq_length = 200"
      ],
      "metadata": {
        "id": "XgqRuoN5z-HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hp = HParams()"
      ],
      "metadata": {
        "id": "TjGjrJYi0Daw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing & Loading Dataset"
      ],
      "metadata": {
        "id": "wgm8a0JWVAI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def max_size(data):\n",
        "    \"\"\"larger sequence length in the data set\"\"\"\n",
        "    sizes = [len(seq) for seq in data]\n",
        "    return max(sizes)\n",
        "\n",
        "def purify(strokes):\n",
        "    \"\"\"removes to small or too long sequences + removes large gaps\"\"\"\n",
        "    data = []\n",
        "    for seq in strokes:\n",
        "        if seq.shape[0] <= hp.max_seq_length and seq.shape[0] > 10:\n",
        "            seq = np.minimum(seq, 1000)\n",
        "            seq = np.maximum(seq, -1000)\n",
        "            seq = np.array(seq, dtype=np.float32)\n",
        "            data.append(seq)\n",
        "    return data\n",
        "\n",
        "def calculate_normalizing_scale_factor(strokes):\n",
        "    \"\"\"Calculate the normalizing factor explained in appendix of sketch-rnn.\"\"\"\n",
        "    data = []\n",
        "    for i in range(len(strokes)):\n",
        "        for j in range(len(strokes[i])):\n",
        "            data.append(strokes[i][j, 0])\n",
        "            data.append(strokes[i][j, 1])\n",
        "    data = np.array(data)\n",
        "    return np.std(data)\n",
        "\n",
        "def normalize(strokes):\n",
        "    \"\"\"Normalize entire dataset (delta_x, delta_y) by the scaling factor.\"\"\"\n",
        "    data = []\n",
        "    scale_factor = calculate_normalizing_scale_factor(strokes)\n",
        "    for seq in strokes:\n",
        "        seq[:, 0:2] /= scale_factor\n",
        "        data.append(seq)\n",
        "    return data"
      ],
      "metadata": {
        "id": "NNIR-xA7ULqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = np.load(hp.data_location, encoding='latin1',allow_pickle=True)\n",
        "data = dataset['train']\n",
        "data = purify(data)\n",
        "data = normalize(data)\n",
        "Nmax = max_size(data)"
      ],
      "metadata": {
        "id": "u5ptDp7VUNgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch(batch_size):\n",
        "    batch_idx = np.random.choice(len(data), batch_size)\n",
        "    batch_sequences = [data[idx] for idx in batch_idx]\n",
        "    strokes = []\n",
        "    lengths = []\n",
        "\n",
        "    for seq in batch_sequences:\n",
        "        len_seq = len(seq[:, 0])\n",
        "        new_seq = np.zeros((Nmax, 5))\n",
        "        new_seq[:len_seq, :2] = seq[:, :2]\n",
        "        new_seq[:len_seq - 1, 2] = 1 - seq[:-1, 2]\n",
        "        new_seq[:len_seq, 3] = seq[:, 2]\n",
        "        new_seq[len_seq - 1:, 4] = 1\n",
        "        new_seq[len_seq - 1, 2:4] = 0\n",
        "        lengths.append(len(seq[:, 0]))\n",
        "        strokes.append(new_seq)\n",
        "\n",
        "    # Stack the sequences into a tensor\n",
        "    batch = torch.from_numpy(np.stack(strokes, axis=1)).float()\n",
        "\n",
        "    # Move tensor to GPU if CUDA is available\n",
        "    if use_cuda:\n",
        "        batch = batch.cuda()\n",
        "\n",
        "    return batch, lengths"
      ],
      "metadata": {
        "id": "uig_0PQqUQDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_decay(optimizer):\n",
        "    \"\"\"Decay learning rate by a factor of lr_decay\"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        if param_group['lr']>hp.min_lr:\n",
        "            param_group['lr'] *= hp.lr_decay\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "nsA_UC9eUSeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder & Decoder RNN"
      ],
      "metadata": {
        "id": "tfS4H6u-VEds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        # bidirectional lstm:\n",
        "        self.lstm = nn.LSTM(5, hp.enc_hidden_size, \\\n",
        "            dropout=hp.dropout, bidirectional=True)\n",
        "        # create mu and sigma from lstm's last output:\n",
        "        self.fc_mu = nn.Linear(2*hp.enc_hidden_size, hp.Nz)\n",
        "        self.fc_sigma = nn.Linear(2*hp.enc_hidden_size, hp.Nz)\n",
        "        # active dropout:\n",
        "        self.train()\n",
        "\n",
        "    def forward(self, inputs, batch_size, hidden_cell=None):\n",
        "        if hidden_cell is None:\n",
        "            # then must init with zeros\n",
        "            if use_cuda:\n",
        "                hidden = torch.zeros(2, batch_size, hp.enc_hidden_size).cuda()\n",
        "                cell = torch.zeros(2, batch_size, hp.enc_hidden_size).cuda()\n",
        "            else:\n",
        "                hidden = torch.zeros(2, batch_size, hp.enc_hidden_size)\n",
        "                cell = torch.zeros(2, batch_size, hp.enc_hidden_size)\n",
        "            hidden_cell = (hidden, cell)\n",
        "        _, (hidden,cell) = self.lstm(inputs.float(), hidden_cell)\n",
        "        # hidden is (2, batch_size, hidden_size), we want (batch_size, 2*hidden_size)\n",
        "        hidden_forward, hidden_backward = torch.split(hidden,1,0)\n",
        "        hidden_cat = torch.cat([hidden_forward.squeeze(0), hidden_backward.squeeze(0)],1)\n",
        "        # mu and sigma:\n",
        "        mu = self.fc_mu(hidden_cat)\n",
        "        sigma_hat = self.fc_sigma(hidden_cat)\n",
        "        sigma = torch.exp(sigma_hat/2.)\n",
        "        # N ~ N(0,1)\n",
        "        z_size = mu.size()\n",
        "        if use_cuda:\n",
        "            N = torch.normal(torch.zeros(z_size),torch.ones(z_size)).cuda()\n",
        "        else:\n",
        "            N = torch.normal(torch.zeros(z_size),torch.ones(z_size))\n",
        "        z = mu + sigma*N\n",
        "        # mu and sigma_hat are needed for LKL loss\n",
        "        return z, mu, sigma_hat"
      ],
      "metadata": {
        "id": "xwyo8f310cjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        # to init hidden and cell from z:\n",
        "        self.fc_hc = nn.Linear(hp.Nz, 2*hp.dec_hidden_size)\n",
        "        # unidirectional lstm:\n",
        "        self.lstm = nn.LSTM(hp.Nz+5, hp.dec_hidden_size, dropout=hp.dropout)\n",
        "        # create proba distribution parameters from hiddens:\n",
        "        self.fc_params = nn.Linear(hp.dec_hidden_size,6*hp.M+3)\n",
        "\n",
        "    def forward(self, inputs, z, hidden_cell=None):\n",
        "        if hidden_cell is None:\n",
        "            # then we must init from z\n",
        "            hidden,cell = torch.split(F.tanh(self.fc_hc(z)),hp.dec_hidden_size,1)\n",
        "            hidden_cell = (hidden.unsqueeze(0).contiguous(), cell.unsqueeze(0).contiguous())\n",
        "        outputs,(hidden,cell) = self.lstm(inputs, hidden_cell)\n",
        "        # in training we feed the lstm with the whole input in one shot\n",
        "        # and use all outputs contained in 'outputs', while in generate\n",
        "        # mode we just feed with the last generated sample:\n",
        "        if self.training:\n",
        "            y = self.fc_params(outputs.view(-1, hp.dec_hidden_size))\n",
        "        else:\n",
        "            y = self.fc_params(hidden.view(-1, hp.dec_hidden_size))\n",
        "        # separate pen and mixture params:\n",
        "        params = torch.split(y,6,1)\n",
        "        params_mixture = torch.stack(params[:-1]) # trajectory\n",
        "        params_pen = params[-1] # pen up/down\n",
        "        # identify mixture params:\n",
        "        pi,mu_x,mu_y,sigma_x,sigma_y,rho_xy = torch.split(params_mixture,1,2)\n",
        "        # preprocess params::\n",
        "        if self.training:\n",
        "            len_out = Nmax+1\n",
        "        else:\n",
        "            len_out = 1\n",
        "\n",
        "        pi = F.softmax(pi.transpose(0,1).squeeze(), dim=-1).view(len_out,-1,hp.M)\n",
        "        sigma_x = torch.exp(sigma_x.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
        "        sigma_y = torch.exp(sigma_y.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
        "        rho_xy = torch.tanh(rho_xy.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
        "        mu_x = mu_x.transpose(0,1).squeeze().contiguous().view(len_out,-1,hp.M)\n",
        "        mu_y = mu_y.transpose(0,1).squeeze().contiguous().view(len_out,-1,hp.M)\n",
        "        q = F.softmax(params_pen, dim=-1).view(len_out,-1,3)\n",
        "\n",
        "        return pi,mu_x,mu_y,sigma_x,sigma_y,rho_xy,q,hidden,cell\n"
      ],
      "metadata": {
        "id": "1Tgx7V7y0gbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Model & Training"
      ],
      "metadata": {
        "id": "8TiSHfqyVIVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model():\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "        self.encoder = EncoderRNN().to(self.device)\n",
        "        self.decoder = DecoderRNN().to(self.device)\n",
        "\n",
        "        self.encoder_optimizer = optim.Adam(self.encoder.parameters(), hp.lr)\n",
        "        self.decoder_optimizer = optim.Adam(self.decoder.parameters(), hp.lr)\n",
        "        self.eta_step = hp.eta_min\n",
        "\n",
        "    def make_target(self, batch, lengths):\n",
        "        batch_size = batch.size(1)  # Get batch size\n",
        "        eos = torch.tensor([0, 0, 0, 0, 1], device=self.device).repeat(batch_size, 1).unsqueeze(0)\n",
        "        batch = torch.cat([batch, eos], dim=0)  # Append EOS token\n",
        "\n",
        "         # Move tensors to the same device\n",
        "        mask = torch.zeros(Nmax + 1, batch_size, device=self.device)  # Ensure mask is on the right device\n",
        "        lengths_tensor = torch.tensor(lengths, device=self.device)  # Move lengths to the same device\n",
        "        indices = torch.arange(Nmax + 1, device=self.device).unsqueeze(1)  # Move arange tensor to the right device\n",
        "        max_len = min(Nmax + 1, max(lengths))  # Ensure max length doesn't exceed Nmax+1\n",
        "        mask[:max_len, torch.arange(batch_size, device=self.device)] = (indices[:max_len] < lengths_tensor).float()\n",
        "\n",
        "\n",
        "        # Extract components efficiently\n",
        "        dx = batch[:, :, 0].unsqueeze(-1).expand(-1, -1, hp.M)\n",
        "        dy = batch[:, :, 1].unsqueeze(-1).expand(-1, -1, hp.M)\n",
        "        p = batch[:, :, 2:5]  # Direct slicing instead of stacking\n",
        "\n",
        "        return mask, dx, dy, p\n",
        "\n",
        "    def train(self, epoch):\n",
        "      self.encoder.train()\n",
        "      self.decoder.train()\n",
        "\n",
        "    # Load batch data\n",
        "      batch, lengths = make_batch(hp.batch_size)\n",
        "      batch = batch.to(self.device)  # Move batch to correct device\n",
        "\n",
        "    # Encode\n",
        "      z, self.mu, self.sigma = self.encoder(batch, hp.batch_size)\n",
        "\n",
        "    # Create start of sequence (sos)\n",
        "      sos = torch.tensor([0, 0, 1, 0, 0], device=self.device).repeat(hp.batch_size, 1).unsqueeze(0)\n",
        "\n",
        "    # Concatenate sos with batch\n",
        "      batch_init = torch.cat([sos, batch], dim=0)\n",
        "\n",
        "    # Expand `z` for concatenation\n",
        "      z_stack = z.unsqueeze(0).expand(Nmax + 1, -1, -1)\n",
        "\n",
        "    # Concatenate inputs\n",
        "      inputs = torch.cat([batch_init, z_stack], dim=2)\n",
        "\n",
        "    # Decode\n",
        "      self.pi, self.mu_x, self.mu_y, self.sigma_x, self.sigma_y, \\\n",
        "        self.rho_xy, self.q, _, _ = self.decoder(inputs, z)\n",
        "\n",
        "    # Prepare targets\n",
        "      mask, dx, dy, p = self.make_target(batch, lengths)\n",
        "\n",
        "    # Zero gradients\n",
        "      self.encoder_optimizer.zero_grad()\n",
        "      self.decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Update eta for KL loss\n",
        "      self.eta_step = 1 - (1 - hp.eta_min) * hp.R\n",
        "\n",
        "    # Compute losses\n",
        "      LKL = self.kullback_leibler_loss()\n",
        "      LR = self.reconstruction_loss(mask, dx, dy, p, epoch)\n",
        "      loss = LR + LKL\n",
        "\n",
        "    # Backpropagation\n",
        "      loss.backward()\n",
        "\n",
        "    # Gradient Clipping\n",
        "      nn.utils.clip_grad_norm_(self.encoder.parameters(), hp.grad_clip)\n",
        "      nn.utils.clip_grad_norm_(self.decoder.parameters(), hp.grad_clip)\n",
        "\n",
        "    # Optimization Step\n",
        "      self.encoder_optimizer.step()\n",
        "      self.decoder_optimizer.step()\n",
        "\n",
        "    # Logging and Learning Rate Decay\n",
        "      if epoch % 1 == 0:\n",
        "        print(f\"Epoch {epoch} | Loss: {loss.item():.4f} | LR: {LR.item():.4f} | LKL: {LKL.item():.4f}\")\n",
        "        self.encoder_optimizer = lr_decay(self.encoder_optimizer)\n",
        "        self.decoder_optimizer = lr_decay(self.decoder_optimizer)\n",
        "\n",
        "    # Conditional Generation and Saving\n",
        "      if epoch % 100 == 0:\n",
        "        self.conditional_generation(epoch)\n",
        "\n",
        "\n",
        "    def bivariate_normal_pdf(self, dx, dy):\n",
        "        z_x = ((dx-self.mu_x)/self.sigma_x)**2\n",
        "        z_y = ((dy-self.mu_y)/self.sigma_y)**2\n",
        "        z_xy = (dx-self.mu_x)*(dy-self.mu_y)/(self.sigma_x*self.sigma_y)\n",
        "        z = z_x + z_y -2*self.rho_xy*z_xy\n",
        "        exp = torch.exp(-z/(2*(1-self.rho_xy**2)))\n",
        "        norm = 2*np.pi*self.sigma_x*self.sigma_y*torch.sqrt(1-self.rho_xy**2)\n",
        "        return exp/norm\n",
        "\n",
        "    def reconstruction_loss(self, mask, dx, dy, p, epoch):\n",
        "        pdf = self.bivariate_normal_pdf(dx, dy)\n",
        "        LS = -torch.sum(mask*torch.log(1e-5+torch.sum(self.pi * pdf, 2)))\\\n",
        "            /float(Nmax*hp.batch_size)\n",
        "        LP = -torch.sum(p*torch.log(self.q))/float(Nmax*hp.batch_size)\n",
        "        return LS+LP\n",
        "\n",
        "    def kullback_leibler_loss(self):\n",
        "        LKL = -0.5*torch.sum(1+self.sigma-self.mu**2-torch.exp(self.sigma))\\\n",
        "            /float(hp.Nz*hp.batch_size)\n",
        "        if use_cuda:\n",
        "            KL_min = torch.Tensor([hp.KL_min]).cuda().detach()\n",
        "        else:\n",
        "            KL_min = torch.Tensor([hp.KL_min]).detach()\n",
        "        return hp.wKL*self.eta_step * torch.max(LKL,KL_min)\n",
        "\n",
        "    def save(self, epoch):\n",
        "        sel = np.random.rand()\n",
        "        torch.save(self.encoder.state_dict(), \\\n",
        "            'encoderRNN_sel_%3f_epoch_%d.pth' % (sel,epoch))\n",
        "        torch.save(self.decoder.state_dict(), \\\n",
        "            'decoderRNN_sel_%3f_epoch_%d.pth' % (sel,epoch))\n",
        "\n",
        "    def load(self, encoder_name, decoder_name):\n",
        "        saved_encoder = torch.load(encoder_name)\n",
        "        saved_decoder = torch.load(decoder_name)\n",
        "        self.encoder.load_state_dict(saved_encoder)\n",
        "        self.decoder.load_state_dict(saved_decoder)\n",
        "\n",
        "    def conditional_generation(self, epoch):\n",
        "        batch,lengths = make_batch(1)\n",
        "        # should remove dropouts:\n",
        "        self.encoder.train(False)\n",
        "        self.decoder.train(False)\n",
        "        # encode:\n",
        "        z, _, _ = self.encoder(batch, 1)\n",
        "        if use_cuda:\n",
        "            sos = torch.Tensor([0,0,1,0,0]).view(1,1,-1).cuda()\n",
        "        else:\n",
        "            sos = torch.Tensor([0,0,1,0,0]).view(1,1,-1)\n",
        "        s = sos\n",
        "        seq_x = []\n",
        "        seq_y = []\n",
        "        seq_z = []\n",
        "        hidden_cell = None\n",
        "        for i in range(Nmax):\n",
        "            input = torch.cat([s,z.unsqueeze(0)],2)\n",
        "            # decode:\n",
        "            self.pi, self.mu_x, self.mu_y, self.sigma_x, self.sigma_y, \\\n",
        "                self.rho_xy, self.q, hidden, cell = \\\n",
        "                    self.decoder(input, z, hidden_cell)\n",
        "            hidden_cell = (hidden, cell)\n",
        "            # sample from parameters:\n",
        "            s, dx, dy, pen_down, eos = self.sample_next_state()\n",
        "            #------\n",
        "            seq_x.append(dx)\n",
        "            seq_y.append(dy)\n",
        "            seq_z.append(pen_down)\n",
        "            if eos:\n",
        "                print(i)\n",
        "                break\n",
        "        # visualize result:\n",
        "        x_sample = np.cumsum(seq_x, 0)\n",
        "        y_sample = np.cumsum(seq_y, 0)\n",
        "        z_sample = np.array(seq_z)\n",
        "        sequence = np.stack([x_sample,y_sample,z_sample]).T\n",
        "        make_image(sequence, epoch)\n",
        "\n",
        "    def sample_next_state(self):\n",
        "      def adjust_temp(pi_pdf):\n",
        "        pi_pdf = np.log(pi_pdf)/hp.temperature\n",
        "        pi_pdf -= pi_pdf.max()\n",
        "        pi_pdf = np.exp(pi_pdf)\n",
        "        pi_pdf /= pi_pdf.sum()\n",
        "        return pi_pdf\n",
        "\n",
        "    # get mixture indice:\n",
        "      pi = self.pi.data[0,0,:].cpu().numpy()  # Added .cpu() before .numpy()\n",
        "      pi = adjust_temp(pi)\n",
        "      pi_idx = np.random.choice(hp.M, p=pi)\n",
        "    # get pen state:\n",
        "      q = self.q.data[0,0,:].cpu().numpy()  # Added .cpu() before .numpy()\n",
        "      q = adjust_temp(q)\n",
        "      q_idx = np.random.choice(3, p=q)\n",
        "    # get mixture params:\n",
        "      mu_x = self.mu_x.data[0,0,pi_idx].cpu().numpy()  # Added .cpu().numpy()\n",
        "      mu_y = self.mu_y.data[0,0,pi_idx].cpu().numpy()  # Added .cpu().numpy()\n",
        "      sigma_x = self.sigma_x.data[0,0,pi_idx].cpu().numpy()  # Added .cpu().numpy()\n",
        "      sigma_y = self.sigma_y.data[0,0,pi_idx].cpu().numpy()  # Added .cpu().numpy()\n",
        "      rho_xy = self.rho_xy.data[0,0,pi_idx].cpu().numpy()  # Added .cpu().numpy()\n",
        "      x,y = sample_bivariate_normal(mu_x,mu_y,sigma_x,sigma_y,rho_xy,greedy=False)\n",
        "      next_state = torch.zeros(5)\n",
        "      next_state[0] = x\n",
        "      next_state[1] = y\n",
        "      next_state[q_idx+2] = 1\n",
        "      if use_cuda:\n",
        "        return next_state.cuda().view(1,1,-1),x,y,q_idx==1,q_idx==2\n",
        "      else:\n",
        "        return next_state.view(1,1,-1),x,y,q_idx==1,q_idx==2"
      ],
      "metadata": {
        "id": "cVOQRgMhTbGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Miscellaneous Functions"
      ],
      "metadata": {
        "id": "Z9aRkCfPVfRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_bivariate_normal(mu_x, mu_y, sigma_x, sigma_y, rho_xy, greedy=False):\n",
        "    # inputs are now numpy values, not tensors\n",
        "    if greedy:\n",
        "        return mu_x, mu_y\n",
        "    mean = [mu_x, mu_y]\n",
        "    sigma_x *= np.sqrt(hp.temperature)\n",
        "    sigma_y *= np.sqrt(hp.temperature)\n",
        "    cov = [[sigma_x * sigma_x, rho_xy * sigma_x * sigma_y],\n",
        "           [rho_xy * sigma_x * sigma_y, sigma_y * sigma_y]]\n",
        "    x = np.random.multivariate_normal(mean, cov, 1)\n",
        "    return x[0][0], x[0][1]"
      ],
      "metadata": {
        "id": "dNvM2c0KUZIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_image(sequence, epoch, name='_output_'):\n",
        "    \"\"\"plot drawing with separated strokes\"\"\"\n",
        "    strokes = np.split(sequence, np.where(sequence[:,2]>0)[0]+1)\n",
        "    fig = plt.figure()\n",
        "    ax1 = fig.add_subplot(111)\n",
        "    for s in strokes:\n",
        "        plt.plot(s[:,0],-s[:,1])\n",
        "    canvas = plt.get_current_fig_manager().canvas\n",
        "    canvas.draw()\n",
        "\n",
        "    # Updated method to get image data from canvas\n",
        "    # In newer matplotlib versions, tostring_rgb() is replaced with buffer_rgba()\n",
        "    # Then we need to convert RGBA to RGB\n",
        "    width, height = canvas.get_width_height()\n",
        "\n",
        "    # Try the newer method first\n",
        "    try:\n",
        "        buffer = canvas.buffer_rgba()\n",
        "        image_array = np.asarray(buffer)\n",
        "        # Convert RGBA to RGB\n",
        "        pil_image = PIL.Image.fromarray(image_array[:, :, :3])\n",
        "    except:\n",
        "        # Fallback for older versions, try different methods\n",
        "        try:\n",
        "            buffer = canvas.tostring_rgb()\n",
        "            pil_image = PIL.Image.frombytes('RGB', (width, height), buffer)\n",
        "        except:\n",
        "            # Last resort\n",
        "            plt.savefig(f\"{epoch}{name}_temp.png\")\n",
        "            pil_image = PIL.Image.open(f\"{epoch}{name}_temp.png\")\n",
        "\n",
        "    # Save the image\n",
        "    name = str(epoch) + name + '.jpg'\n",
        "    pil_image.save(name, \"JPEG\")\n",
        "    plt.close(\"all\")"
      ],
      "metadata": {
        "id": "QQ3F2PJVUa1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, save_dir='./saved_models', model_name='sketchrnn_apple'):\n",
        "    \"\"\"\n",
        "    Save the trained model weights to disk.\n",
        "\n",
        "    Args:\n",
        "        model: The trained SketchRNN model\n",
        "        save_dir: Directory to save the model files\n",
        "        model_name: Base name for the saved model files\n",
        "\n",
        "    Returns:\n",
        "        tuple: Paths to the saved encoder and decoder files\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Generate timestamp for the filename\n",
        "    import time\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Create filenames\n",
        "    encoder_path = os.path.join(save_dir, f\"{model_name}_encoder_{timestamp}.pth\")\n",
        "    decoder_path = os.path.join(save_dir, f\"{model_name}_decoder_{timestamp}.pth\")\n",
        "\n",
        "    # Save model state dictionaries\n",
        "    torch.save(model.encoder.state_dict(), encoder_path)\n",
        "    torch.save(model.decoder.state_dict(), decoder_path)\n",
        "\n",
        "    print(f\"Model saved successfully:\")\n",
        "    print(f\"- Encoder: {encoder_path}\")\n",
        "    print(f\"- Decoder: {decoder_path}\")\n",
        "\n",
        "    # Also save a config file with hyperparameters\n",
        "    config_path = os.path.join(save_dir, f\"{model_name}_config_{timestamp}.txt\")\n",
        "    with open(config_path, 'w') as f:\n",
        "        for param, value in vars(hp).items():\n",
        "            f.write(f\"{param}: {value}\\n\")\n",
        "\n",
        "    print(f\"- Config: {config_path}\")\n",
        "\n",
        "    return encoder_path, decoder_path"
      ],
      "metadata": {
        "id": "o3TLZ9-aUd_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sketch Generation Function"
      ],
      "metadata": {
        "id": "e5Numi65VM4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_drawing_animation(model, num_frames=None, save_path='drawing_animation.gif', temperature=0.4):\n",
        "    \"\"\"\n",
        "    Generates a GIF animation showing the stroke-by-stroke drawing process from the trained model.\n",
        "\n",
        "    Args:\n",
        "        model: The trained SketchRNN model\n",
        "        num_frames: Maximum number of frames to include (None = all)\n",
        "        save_path: Path to save the output GIF\n",
        "        temperature: Temperature for sampling (lower = more deterministic)\n",
        "    \"\"\"\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.encoder.train(False)\n",
        "    model.decoder.train(False)\n",
        "\n",
        "    # Get a random batch\n",
        "    batch, lengths = make_batch(1)\n",
        "\n",
        "    # Encode\n",
        "    z, _, _ = model.encoder(batch, 1)\n",
        "\n",
        "    # Setup for decoding\n",
        "    if use_cuda:\n",
        "        sos = torch.Tensor([0, 0, 1, 0, 0]).view(1, 1, -1).cuda()\n",
        "    else:\n",
        "        sos = torch.Tensor([0, 0, 1, 0, 0]).view(1, 1, -1)\n",
        "\n",
        "    s = sos\n",
        "    seq_x = []\n",
        "    seq_y = []\n",
        "    seq_z = []\n",
        "    hidden_cell = None\n",
        "\n",
        "    # Store the original temperature\n",
        "    original_temp = hp.temperature\n",
        "    # Set the temperature for generation\n",
        "    hp.temperature = temperature\n",
        "\n",
        "    # Generate the sequence\n",
        "    for i in range(Nmax):\n",
        "        input = torch.cat([s, z.unsqueeze(0)], 2)\n",
        "        # decode:\n",
        "        model.pi, model.mu_x, model.mu_y, model.sigma_x, model.sigma_y, \\\n",
        "            model.rho_xy, model.q, hidden, cell = \\\n",
        "                model.decoder(input, z, hidden_cell)\n",
        "        hidden_cell = (hidden, cell)\n",
        "        # sample from parameters:\n",
        "        s, dx, dy, pen_down, eos = model.sample_next_state()\n",
        "\n",
        "        seq_x.append(dx)\n",
        "        seq_y.append(dy)\n",
        "        seq_z.append(pen_down)\n",
        "\n",
        "        if eos:\n",
        "            print(f\"Drawing completed in {i} steps\")\n",
        "            break\n",
        "\n",
        "    # Cumulative sum for coordinates\n",
        "    x_sample = np.cumsum(seq_x, 0)\n",
        "    y_sample = np.cumsum(seq_y, 0)\n",
        "    z_sample = np.array(seq_z)\n",
        "\n",
        "    # Limit the number of frames if specified\n",
        "    total_frames = len(x_sample)\n",
        "    if num_frames is not None:\n",
        "        if num_frames < total_frames:\n",
        "            # Take evenly spaced frames\n",
        "            indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "            x_sample = x_sample[indices]\n",
        "            y_sample = y_sample[indices]\n",
        "            z_sample = z_sample[indices]\n",
        "            total_frames = num_frames\n",
        "\n",
        "    # Create frames\n",
        "    frames = []\n",
        "    print(f\"Creating animation with {total_frames} frames...\")\n",
        "\n",
        "    fig = plt.figure(figsize=(7, 7), dpi=100)\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    # Find min and max for stable axes\n",
        "    x_min, x_max = min(x_sample), max(x_sample)\n",
        "    y_min, y_max = min(-y_sample), max(-y_sample)\n",
        "\n",
        "    # Add some padding\n",
        "    x_padding = (x_max - x_min) * 0.1\n",
        "    y_padding = (y_max - y_min) * 0.1\n",
        "\n",
        "    for i in range(total_frames):\n",
        "        # Clear the axis for each frame\n",
        "        ax.clear()\n",
        "\n",
        "        # Get stroke end indices\n",
        "        stroke_ends = np.where(z_sample[:i+1])[0] + 1\n",
        "\n",
        "        # Split the sequence by strokes\n",
        "        if len(stroke_ends) > 0:\n",
        "            last_end = 0\n",
        "            for end in stroke_ends:\n",
        "                if end > i:\n",
        "                    end = i + 1\n",
        "                # Plot each stroke separately\n",
        "                if end > last_end:\n",
        "                    ax.plot(x_sample[last_end:end], -y_sample[last_end:end], 'k-', linewidth=2)\n",
        "                last_end = end\n",
        "\n",
        "            # If there's remaining points after the last complete stroke\n",
        "            if last_end <= i:\n",
        "                ax.plot(x_sample[last_end:i+1], -y_sample[last_end:i+1], 'k-', linewidth=2)\n",
        "        else:\n",
        "            # No complete strokes yet\n",
        "            ax.plot(x_sample[:i+1], -y_sample[:i+1], 'k-', linewidth=2)\n",
        "\n",
        "        # Set consistent axes limits for stable animation\n",
        "        margin_x = (x_max - x_min) * 0.2\n",
        "        margin_y = (y_max - y_min) * 0.2\n",
        "        ax.set_xlim(x_min - margin_x, x_max + margin_x)\n",
        "        ax.set_ylim(y_min - margin_y, y_max + margin_y)\n",
        "\n",
        "        # Remove axes for cleaner visualization\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Title showing progress\n",
        "        ax.set_title(f\"Frame {i+1}/{total_frames}\")\n",
        "\n",
        "        # Create canvas for saving\n",
        "        canvas = FigureCanvas(fig)\n",
        "        canvas.draw()\n",
        "\n",
        "        # Convert canvas to image\n",
        "        buf = io.BytesIO()\n",
        "        fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0.1)\n",
        "        buf.seek(0)\n",
        "        img = Image.open(buf)\n",
        "        frames.append(img.copy())\n",
        "        buf.close()\n",
        "\n",
        "    # Save as GIF\n",
        "    duration = 1000 / 30  # 30 fps\n",
        "    frames[0].save(\n",
        "        save_path,\n",
        "        format='GIF',\n",
        "        append_images=frames[1:],\n",
        "        save_all=True,\n",
        "        duration=duration,\n",
        "        loop=0\n",
        "    )\n",
        "\n",
        "    # Restore original temperature\n",
        "    hp.temperature = original_temp\n",
        "\n",
        "    plt.close(fig)\n",
        "    print(f\"Animation saved to {save_path}\")\n",
        "\n",
        "    return save_path"
      ],
      "metadata": {
        "id": "uvAJX0IzTfYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sketch Generation by class"
      ],
      "metadata": {
        "id": "0B0yaVtkVQwx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KAbxyncTW9b"
      },
      "outputs": [],
      "source": [
        "def load_model_by_class(class_name):\n",
        "    \"\"\"\n",
        "    Loads a SketchRNN model for the given class by loading the corresponding encoder and decoder files.\n",
        "\n",
        "    Args:\n",
        "        class_name (str): The class name. Expected values include: 'cat', 'dog', 'airplane', 'apple', 'book'.\n",
        "\n",
        "    Returns:\n",
        "        Model: An instance of Model with loaded encoder and decoder.\n",
        "    \"\"\"\n",
        "    # Construct file paths for the encoder and decoder based on the class name\n",
        "    encoder_file = f\"sketchrnn_{class_name}_encoder.pth\"\n",
        "    decoder_file = f\"sketchrnn_{class_name}_decoder.pth\"\n",
        "\n",
        "    # Instantiate your Model (which internally creates encoder and decoder)\n",
        "    model = Model()\n",
        "\n",
        "    # Load the saved weights into the model's encoder and decoder\n",
        "    model.load(encoder_file, decoder_file)\n",
        "\n",
        "    return model\n",
        "\n",
        "def generate_sketch_by_class(class_name, num_frames=None, save_path=None, temperature=0.4):\n",
        "    \"\"\"\n",
        "    Generates a sketch for the given class by loading the corresponding model files and then calling\n",
        "    the pre-existing generate_drawing_animation function.\n",
        "\n",
        "    Args:\n",
        "        class_name (str): The class name. Expected: 'cat', 'dog', 'airplane', 'apple', 'book'.\n",
        "        num_frames (int, optional): Maximum number of frames to include in the animation.\n",
        "        save_path (str, optional): File path to save the output GIF. If not provided, a default filename will be used.\n",
        "        temperature (float, optional): Temperature for sampling (lower values yield more deterministic output).\n",
        "\n",
        "    Returns:\n",
        "        str: The path where the generated animation GIF is saved.\n",
        "    \"\"\"\n",
        "    # Load the appropriate model for the given class\n",
        "    model = load_model_by_class(class_name)\n",
        "\n",
        "    # Use a default save path if none is provided\n",
        "    if save_path is None:\n",
        "        save_path = f\"{class_name}_sketch.gif\"\n",
        "\n",
        "    # Call your existing drawing animation function with the loaded model\n",
        "    return generate_drawing_animation(model, num_frames=num_frames, save_path=save_path, temperature=temperature)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sketch_by_class('book')"
      ],
      "metadata": {
        "id": "oTbejLmaaRhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UcHVVSFkaW_F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}