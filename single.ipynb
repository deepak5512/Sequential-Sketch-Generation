{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the Required Libraries"
      ],
      "metadata": {
        "id": "97NXFkp6WvbM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf-tymTrjF9j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import os\n",
        "import time\n",
        "import io\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p sketch_data\n",
        "!mkdir -p /content/cRNN_logs\n",
        "!mkdir -p /content/cRNN_checkpoints"
      ],
      "metadata": {
        "id": "RCUf20jbjPCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EcYLrQZjWZ6",
        "outputId": "11cfce6b-7262-41c7-e75c-01cc452a6dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Required Hyperparameters"
      ],
      "metadata": {
        "id": "uX7u04aGW2fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HParams():\n",
        "    def __init__(self):\n",
        "        # Data and Classes\n",
        "        self.classes = ['cat', 'apple', 'airplane','candle','alarm clock' ] # Ensure you have data\n",
        "        self.data_location = '/content/sketch_data'  # <<<--- POINT TO YOUR DATA DIRECTORY\n",
        "        self.log_dir = '/content/cRNN_logs' # <<<--- TensorBoard logs for this model\n",
        "        self.checkpoint_dir = '/content/cRNN_checkpoints' # <<<--- Checkpoints for this model\n",
        "        self.max_seq_length = 200 # Max sequence length to keep\n",
        "        self.min_seq_length = 10  # Min sequence length to keep\n",
        "\n",
        "        # Model Architecture\n",
        "        self.dec_hidden_size = 512 # Decoder RNN hidden size\n",
        "        self.M = 20                # Number of mixture components\n",
        "        self.class_embedding_size = 64 # Size of the learned class embedding vector\n",
        "\n",
        "        # Training Parameters\n",
        "        self.batch_size = 100\n",
        "        self.lr = 0.001\n",
        "        self.scheduler_factor = 0.5 # Factor to reduce LR by\n",
        "        self.scheduler_patience = 10 # Epochs to wait for improvement before reducing LR\n",
        "        self.grad_clip = 1.0       # Gradient clipping threshold\n",
        "        self.temperature = 0.4     # Sampling temperature\n",
        "        self.dropout = 0.5         # Can experiment with dropout rate (0.9 might be too high without VAE)\n",
        "\n",
        "        # Data Splitting\n",
        "        self.validation_split = 0.15\n",
        "        self.test_split = 0.15\n",
        "\n",
        "        # Training Control\n",
        "        self.num_epochs = 20 # <<<--- Adjust epochs\n",
        "        self.epochs_til_checkpoint = 500\n",
        "        self.epochs_til_validation = 20\n"
      ],
      "metadata": {
        "id": "H37SkrdejkMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hp = HParams()"
      ],
      "metadata": {
        "id": "x08rRBsojxlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing & Loading Data"
      ],
      "metadata": {
        "id": "y0Uur8diW5-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def max_size(data):\n",
        "    \"\"\"Calculates the longest sequence length in the dataset.\"\"\"\n",
        "    sizes = [len(seq) for seq in data]\n",
        "    return max(sizes) if sizes else 0\n",
        "\n",
        "def purify(strokes):\n",
        "    \"\"\"Removes sequences that are too short or too long.\"\"\"\n",
        "    data = []\n",
        "    for seq in strokes:\n",
        "        if hp.min_seq_length <= seq.shape[0] <= hp.max_seq_length:\n",
        "            seq = np.minimum(seq, 1000)\n",
        "            seq = np.maximum(seq, -1000)\n",
        "            seq = np.array(seq, dtype=np.float32)\n",
        "            data.append(seq)\n",
        "    return data\n",
        "\n",
        "def calculate_normalizing_scale_factor(strokes):\n",
        "    \"\"\"Calculates the stddev of stroke displacements used for normalization.\"\"\"\n",
        "    all_displacements = []\n",
        "    for seq in strokes:\n",
        "        if len(seq) > 0:\n",
        "             all_displacements.extend(seq[:, 0])\n",
        "             all_displacements.extend(seq[:, 1])\n",
        "    if not all_displacements: return 1.0\n",
        "    return np.std(np.array(all_displacements))\n",
        "\n",
        "def normalize(strokes):\n",
        "    \"\"\"Normalizes stroke displacements and returns data + scale factor.\"\"\"\n",
        "    all_sequences = [seq for seq, _ in strokes]\n",
        "    scale_factor = calculate_normalizing_scale_factor(all_sequences)\n",
        "    if scale_factor < 1e-6: scale_factor = 1.0\n",
        "\n",
        "    normalized_data = []\n",
        "    for seq, class_index in strokes:\n",
        "        normalized_seq = seq.copy()\n",
        "        normalized_seq[:, 0:2] /= scale_factor\n",
        "        normalized_data.append((normalized_seq, class_index))\n",
        "    return normalized_data, scale_factor"
      ],
      "metadata": {
        "id": "WHPGlJ0Qj0ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading and preprocessing data...\")\n",
        "all_data_raw = []\n",
        "class_dict = {class_name: i for i, class_name in enumerate(hp.classes)}\n",
        "num_classes = len(hp.classes)\n",
        "os.makedirs(hp.data_location, exist_ok=True)\n",
        "\n",
        "for class_name in hp.classes:\n",
        "    file_path = os.path.join(hp.data_location, f'sketchrnn_{class_name}.npz')\n",
        "    loaded_count = 0\n",
        "    try:\n",
        "        dataset = np.load(file_path, encoding='latin1', allow_pickle=True)\n",
        "        class_index = class_dict[class_name]\n",
        "        for split in ['train', 'valid', 'test']:\n",
        "            if split in dataset:\n",
        "                split_data = dataset[split]\n",
        "                purified_data = purify(split_data)\n",
        "                for seq in purified_data:\n",
        "                    all_data_raw.append((seq, class_index))\n",
        "                loaded_count += len(purified_data)\n",
        "        print(f\"Loaded {loaded_count} sequences for class '{class_name}'\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "        print(f\"Warning: File not found for class {class_name} at {file_path}\")\n",
        "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or processing file {file_path}: {e}\")\n",
        "\n",
        "if not all_data_raw:\n",
        "    raise ValueError(\"No data loaded. Please check `hp.data_location` and ensure .npz files exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZsyve-9j5Bt",
        "outputId": "2d9a5ddf-83f7-4315-d3cf-abd4454f4690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "Warning: File not found for class cat at /content/sketch_data/sketchrnn_cat.npz\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "Warning: File not found for class apple at /content/sketch_data/sketchrnn_apple.npz\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "Loaded 75000 sequences for class 'airplane'\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "Warning: File not found for class candle at /content/sketch_data/sketchrnn_candle.npz\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "Warning: File not found for class alarm clock at /content/sketch_data/sketchrnn_alarm clock.npz\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_data_normalized, data_scale_factor = normalize(all_data_raw)\n",
        "print(f\"Total sequences after purification: {len(all_data_normalized)}\")\n",
        "print(f\"Data normalization scale factor: {data_scale_factor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1GRHENnkHhG",
        "outputId": "fb30a8cf-7734-4941-f4b1-c666c59dca11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sequences after purification: 75000\n",
            "Data normalization scale factor: 53.127464294433594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, temp_data = train_test_split(\n",
        "    all_data_normalized,\n",
        "    test_size=(hp.validation_split + hp.test_split),\n",
        "    random_state=42,\n",
        "    stratify=[item[1] for item in all_data_normalized]\n",
        ")\n",
        "validation_data, test_data = train_test_split(\n",
        "    temp_data,\n",
        "    test_size=hp.test_split / (hp.validation_split + hp.test_split),\n",
        "    random_state=42,\n",
        "    stratify=[item[1] for item in temp_data]\n",
        ")\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(validation_data)}\")\n",
        "print(f\"Test samples: {len(test_data)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFuz8UK2kMHq",
        "outputId": "6cf6b5e9-1ef1-45c0-c19b-a4db72ff017c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 52500\n",
            "Validation samples: 11250\n",
            "Test samples: 11250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Nmax = 0\n",
        "if train_data: Nmax = max(Nmax, max_size([item[0] for item in train_data]))\n",
        "if validation_data: Nmax = max(Nmax, max_size([item[0] for item in validation_data]))\n",
        "if test_data: Nmax = max(Nmax, max_size([item[0] for item in test_data]))\n",
        "print(f\"Nmax (max sequence length after processing): {Nmax}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvmGYMiTkRHs",
        "outputId": "11193d59-10f9-418e-b9cf-82ad01a39113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nmax (max sequence length after processing): 99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch(batch_size, dataset, Nmax, device):\n",
        "    \"\"\"Creates a batch of sequences with padding and class indices.\"\"\"\n",
        "    num_data = len(dataset)\n",
        "    if num_data == 0: return None, None, None\n",
        "    actual_batch_size = min(batch_size, num_data)\n",
        "    batch_indices = np.random.choice(num_data, actual_batch_size, replace=False)\n",
        "    batch_sequences_with_class = [dataset[idx] for idx in batch_indices]\n",
        "\n",
        "    strokes = []\n",
        "    lengths = []\n",
        "    class_indices = []\n",
        "\n",
        "    for seq, class_index in batch_sequences_with_class:\n",
        "        len_seq = len(seq)\n",
        "        new_seq = np.zeros((Nmax, 5), dtype=np.float32)\n",
        "        new_seq[:len_seq, :2] = seq[:, :2]\n",
        "        new_seq[:len_seq-1, 2] = 1 - seq[:-1, 2] # p1: pen down\n",
        "        new_seq[:len_seq, 3] = seq[:, 2]       # p2: end of stroke\n",
        "        new_seq[len_seq-1:, 4] = 1             # p3: end of drawing\n",
        "        new_seq[len_seq-1, 2:4] = 0\n",
        "\n",
        "        lengths.append(len_seq)\n",
        "        strokes.append(new_seq)\n",
        "        class_indices.append(class_index)\n",
        "\n",
        "    batch_tensor = torch.from_numpy(np.stack(strokes, axis=1)).to(device)\n",
        "    class_indices_tensor = torch.tensor(class_indices, dtype=torch.long, device=device)\n",
        "\n",
        "    # Return lengths as a Python list\n",
        "    return batch_tensor, lengths, class_indices_tensor"
      ],
      "metadata": {
        "id": "kpNjnWz2kVf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder RNN Model"
      ],
      "metadata": {
        "id": "N7QZSDleXB7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalDecoderRNN(nn.Module):\n",
        "    def __init__(self, num_classes, hp):\n",
        "        \"\"\"\n",
        "        Conditional RNN Decoder module.\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): The total number of classes for embedding.\n",
        "            hp (HParams): Hyperparameters object.\n",
        "        \"\"\"\n",
        "        super(ConditionalDecoderRNN, self).__init__()\n",
        "        self.hp = hp\n",
        "\n",
        "        # Class embedding layer\n",
        "        self.embedding = nn.Embedding(num_classes, hp.class_embedding_size)\n",
        "\n",
        "        # Layer to initialize LSTM hidden/cell states from class embedding\n",
        "        # Input: class_embedding_size\n",
        "        # Output: 2 * dec_hidden_size (for hidden and cell)\n",
        "        self.fc_init_hc = nn.Linear(hp.class_embedding_size, 2 * hp.dec_hidden_size)\n",
        "\n",
        "        # Unidirectional LSTM\n",
        "        # Input at each step: 5 (stroke features) + class_embedding_size\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=5 + hp.class_embedding_size,\n",
        "            hidden_size=hp.dec_hidden_size,\n",
        "            num_layers=1, # Standard uses 1 layer here\n",
        "            dropout=hp.dropout,\n",
        "            batch_first=False # Input shape: (seq_len, batch, input_size)\n",
        "        )\n",
        "\n",
        "        # Output layer for Gaussian Mixture Model (GMM) parameters\n",
        "        # Input: dec_hidden_size\n",
        "        # Output: 6*M (pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy) + 3 (pen states q)\n",
        "        self.fc_params = nn.Linear(hp.dec_hidden_size, 6 * hp.M + 3)\n",
        "\n",
        "        # Set module to training mode initially\n",
        "        self.train()\n",
        "\n",
        "    def forward(self, inputs, class_indices, hidden_cell=None):\n",
        "        \"\"\"\n",
        "        Forward pass of the conditional decoder.\n",
        "\n",
        "        Args:\n",
        "            inputs (Tensor): Input sequence (seq_len, batch_size, 5 + class_embedding_size).\n",
        "                             Note: Should already contain concatenated stroke and class embedding.\n",
        "            class_indices (Tensor): Class indices for the batch (batch_size). Used for init state if hidden_cell is None.\n",
        "            hidden_cell (tuple, optional): LSTM hidden and cell states from previous step. Defaults to None (init from class).\n",
        "\n",
        "        Returns:\n",
        "            pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q: Output distribution parameters.\n",
        "            hidden, cell: Last LSTM hidden and cell states.\n",
        "        \"\"\"\n",
        "        # 1. Get Class Embeddings (only needed for init state if hidden_cell is None)\n",
        "        #    The embedding should already be part of the `inputs` tensor for subsequent steps.\n",
        "        if hidden_cell is None:\n",
        "            embedded_init = self.embedding(class_indices) # Shape: (batch_size, class_embedding_size)\n",
        "            # Use fc_init_hc to generate initial hidden and cell states from class embedding\n",
        "            hidden_cell_init = F.tanh(self.fc_init_hc(embedded_init)) # Shape: (batch_size, 2 * dec_hidden_size)\n",
        "            # Split into hidden and cell\n",
        "            hidden, cell = torch.split(hidden_cell_init, self.hp.dec_hidden_size, dim=1)\n",
        "            # Reshape for LSTM (num_layers, batch_size, dec_hidden_size)\n",
        "            hidden_cell = (hidden.unsqueeze(0).contiguous(), cell.unsqueeze(0).contiguous())\n",
        "\n",
        "        # 2. Pass sequence through LSTM\n",
        "        # inputs shape: (seq_len, batch_size, 5 + class_embedding_size)\n",
        "        outputs, (hidden, cell) = self.lstm(inputs, hidden_cell)\n",
        "\n",
        "        # 3. Process outputs for GMM parameters\n",
        "        # Reshape outputs to (seq_len * batch_size, dec_hidden_size)\n",
        "        output_reshaped = outputs.view(-1, self.hp.dec_hidden_size)\n",
        "        y = self.fc_params(output_reshaped)\n",
        "\n",
        "        # 4. Separate and reshape GMM parameters (Identical logic to SketchRNN decoder)\n",
        "        len_out = outputs.size(0)\n",
        "        batch_s = outputs.size(1)\n",
        "        params = torch.split(y, 6, dim=1)\n",
        "        params_mixture = torch.stack(params[:-1], dim=0)\n",
        "        params_pen = params[-1]\n",
        "        pi_logits, mu_x_raw, mu_y_raw, sigma_x_log, sigma_y_log, rho_xy_tanh = torch.split(params_mixture, 1, dim=2)\n",
        "\n",
        "        pi = F.softmax(pi_logits.squeeze(2).transpose(0, 1), dim=-1).view(len_out, batch_s, self.hp.M)\n",
        "        sigma_x = torch.exp(sigma_x_log.squeeze(2).transpose(0, 1)).view(len_out, batch_s, self.hp.M)\n",
        "        sigma_y = torch.exp(sigma_y_log.squeeze(2).transpose(0, 1)).view(len_out, batch_s, self.hp.M)\n",
        "        rho_xy = torch.tanh(rho_xy_tanh.squeeze(2).transpose(0, 1)).view(len_out, batch_s, self.hp.M)\n",
        "        mu_x = mu_x_raw.squeeze(2).transpose(0, 1).contiguous().view(len_out, batch_s, self.hp.M)\n",
        "        mu_y = mu_y_raw.squeeze(2).transpose(0, 1).contiguous().view(len_out, batch_s, self.hp.M)\n",
        "        q = F.softmax(params_pen, dim=-1).view(len_out, batch_s, 3)\n",
        "\n",
        "        return pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q, hidden, cell"
      ],
      "metadata": {
        "id": "8-Hc4VACkYzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Model"
      ],
      "metadata": {
        "id": "TpJHzkzlXE0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model():\n",
        "    def __init__(self, num_classes, hp, device):\n",
        "        self.hp = hp\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Only the Decoder is needed\n",
        "        self.decoder = ConditionalDecoderRNN(num_classes, hp).to(device)\n",
        "\n",
        "        self.params = self.decoder.parameters() # Parameters are only from the decoder\n",
        "        self.optimizer = optim.Adam(self.params, hp.lr)\n",
        "        self.scheduler = ReduceLROnPlateau(self.optimizer, 'min', factor=hp.scheduler_factor, patience=hp.scheduler_patience, verbose=True)\n",
        "        self.best_val_loss = float('inf')\n",
        "\n",
        "    def make_target(self, batch, lengths, Nmax):\n",
        "        \"\"\"Prepares target tensors (mask, dx, dy, p) for loss calculation.\n",
        "           Target length is Nmax (input is SOS + sequence[:Nmax-1]).\n",
        "        \"\"\"\n",
        "        batch_size = batch.size(1)\n",
        "        # Target sequence includes the actual strokes up to Nmax\n",
        "        target_batch = batch[:Nmax] # Shape (Nmax, B, 5)\n",
        "\n",
        "        mask = torch.zeros(Nmax, batch_size, device=self.device) # Mask for Nmax steps\n",
        "        for i in range(batch_size):\n",
        "            # Mask includes up to the last actual point in the sequence (max Nmax points)\n",
        "            mask_len = min(lengths[i], Nmax)\n",
        "            mask[:mask_len, i] = 1\n",
        "\n",
        "        # Targets need to match the Nmax sequence length\n",
        "        dx = target_batch[:, :, 0].unsqueeze(-1).repeat(1, 1, self.hp.M) # Shape: (Nmax, B, M)\n",
        "        dy = target_batch[:, :, 1].unsqueeze(-1).repeat(1, 1, self.hp.M) # Shape: (Nmax, B, M)\n",
        "        p = target_batch[:, :, 2:5] # Shape: (Nmax, B, 3) - Targets for p1, p2, p3\n",
        "\n",
        "        return mask, dx, dy, p\n",
        "\n",
        "    def train_epoch(self, epoch, dataset, Nmax):\n",
        "        \"\"\"Runs one training epoch.\"\"\"\n",
        "        self.decoder.train() # Set decoder to training mode\n",
        "        total_loss = 0\n",
        "        num_batches = len(dataset) // self.hp.batch_size\n",
        "        if num_batches == 0:\n",
        "            print(\"Warning: Not enough data for a single batch in training set.\")\n",
        "            return float('inf')\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            # --- Get Batch ---\n",
        "            # batch shape: (Nmax, B, 5), lengths: list[int], class_indices: (B,)\n",
        "            batch, lengths, class_indices = make_batch(self.hp.batch_size, dataset, Nmax, self.device)\n",
        "            if batch is None: continue\n",
        "            current_batch_size = batch.size(1)\n",
        "\n",
        "            # --- Prepare Decoder Input (Teacher Forcing) ---\n",
        "            # Input is SOS followed by the first (Nmax-1) true strokes\n",
        "            sos = torch.tensor([0, 0, 1, 0, 0], device=self.device, dtype=torch.float32).repeat(current_batch_size, 1).unsqueeze(0)\n",
        "            # Shifted sequence: Use SOS and ground truth strokes as input\n",
        "            decoder_input_stroke = torch.cat([sos, batch[:Nmax-1, :, :]], dim=0) # Shape (Nmax, B, 5)\n",
        "\n",
        "            # Get class embeddings repeated for sequence length Nmax\n",
        "            embedded = self.decoder.embedding(class_indices) # Shape (B, EmbSize)\n",
        "            embedded_expanded = embedded.unsqueeze(0).expand(Nmax, -1, -1) # Shape (Nmax, B, EmbSize)\n",
        "\n",
        "            # Concatenate stroke input with class embedding\n",
        "            decoder_input = torch.cat([decoder_input_stroke, embedded_expanded], dim=2) # Shape (Nmax, B, 5 + EmbSize)\n",
        "\n",
        "            # --- Forward Pass ---\n",
        "            self.optimizer.zero_grad()\n",
        "            # Initialize hidden state using class_indices, pass the full input sequence\n",
        "            pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q, _, _ = self.decoder(decoder_input, class_indices)\n",
        "\n",
        "            # --- Calculate Loss ---\n",
        "            # Targets are the original sequence (batch) up to Nmax steps\n",
        "            mask, dx, dy, p = self.make_target(batch, lengths, Nmax)\n",
        "            loss = self.reconstruction_loss(mask, dx, dy, p, pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q)\n",
        "\n",
        "            # --- Backward Pass & Optimize ---\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.params, self.hp.grad_clip)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"Epoch {epoch} | Train Loss: {avg_loss:.4f}\")\n",
        "        # --- Log to TensorBoard ---\n",
        "        writer.add_scalar('Loss/Train', avg_loss, epoch)\n",
        "        writer.add_scalar('Params/Learning_Rate', self.optimizer.param_groups[0]['lr'], epoch)\n",
        "\n",
        "        return avg_loss\n",
        "\n",
        "    def evaluate(self, dataset, Nmax):\n",
        "        \"\"\"Evaluates the model on a dataset (e.g., validation).\"\"\"\n",
        "        self.decoder.eval() # Set to evaluation mode\n",
        "        total_loss = 0\n",
        "        num_batches = len(dataset) // self.hp.batch_size\n",
        "        if num_batches == 0:\n",
        "             print(\"Warning: Not enough data for a single batch in validation set.\")\n",
        "             return float('inf')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(num_batches):\n",
        "                batch, lengths, class_indices = make_batch(self.hp.batch_size, dataset, Nmax, self.device)\n",
        "                if batch is None: continue\n",
        "                current_batch_size = batch.size(1)\n",
        "\n",
        "                # Prepare inputs (same teacher forcing as training for evaluation loss)\n",
        "                sos = torch.tensor([0, 0, 1, 0, 0], device=self.device, dtype=torch.float32).repeat(current_batch_size, 1).unsqueeze(0)\n",
        "                decoder_input_stroke = torch.cat([sos, batch[:Nmax-1, :, :]], dim=0)\n",
        "                embedded = self.decoder.embedding(class_indices)\n",
        "                embedded_expanded = embedded.unsqueeze(0).expand(Nmax, -1, -1)\n",
        "                decoder_input = torch.cat([decoder_input_stroke, embedded_expanded], dim=2)\n",
        "\n",
        "                # Forward pass\n",
        "                pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q, _, _ = self.decoder(decoder_input, class_indices)\n",
        "\n",
        "                # Loss calculation\n",
        "                mask, dx, dy, p = self.make_target(batch, lengths, Nmax)\n",
        "                loss = self.reconstruction_loss(mask, dx, dy, p, pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"--- Validation | Loss: {avg_loss:.4f} ---\")\n",
        "        # --- Log to TensorBoard ---\n",
        "        writer.add_scalar('Loss/Validation', avg_loss, epoch) # Use current global epoch\n",
        "\n",
        "        return avg_loss\n",
        "\n",
        "    # --- Loss Function (Identical to SketchRNN reconstruction loss) ---\n",
        "    def bivariate_normal_pdf(self, dx, dy, mu_x, mu_y, sigma_x, sigma_y, rho_xy):\n",
        "        sigma_x = torch.clamp(sigma_x, min=1e-5)\n",
        "        sigma_y = torch.clamp(sigma_y, min=1e-5)\n",
        "        rho_xy = torch.clamp(rho_xy, min=-1.0 + 1e-5, max=1.0 - 1e-5)\n",
        "        norm1 = dx - mu_x\n",
        "        norm2 = dy - mu_y\n",
        "        s1s2 = sigma_x * sigma_y\n",
        "        z = (norm1 / sigma_x)**2 + (norm2 / sigma_y)**2 - 2 * rho_xy * norm1 * norm2 / s1s2\n",
        "        rho_sq = rho_xy**2\n",
        "        sqrt_term = torch.clamp(1.0 - rho_sq, min=1e-5)\n",
        "        log_pdf = -z / (2 * sqrt_term) - torch.log(2 * np.pi * s1s2 * torch.sqrt(sqrt_term))\n",
        "        return log_pdf\n",
        "\n",
        "    def reconstruction_loss(self, mask, dx, dy, p, pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q):\n",
        "        log_pdf_vals = self.bivariate_normal_pdf(dx, dy, mu_x, mu_y, sigma_x, sigma_y, rho_xy)\n",
        "        log_pi = torch.log(torch.clamp(pi, min=1e-5))\n",
        "        log_likelihood_stroke = torch.logsumexp(log_pi + log_pdf_vals, dim=2)\n",
        "        log_pen_likelihood = torch.sum(p * torch.log(torch.clamp(q, min=1e-5)), dim=2)\n",
        "        masked_log_likelihood = mask * (log_likelihood_stroke + log_pen_likelihood)\n",
        "        total_elements = torch.sum(mask)\n",
        "        if total_elements == 0: return torch.tensor(0.0, device=self.device)\n",
        "        loss_recon = -torch.sum(masked_log_likelihood) / total_elements\n",
        "        return loss_recon\n",
        "\n",
        "    # --- Checkpointing (Simplified for single decoder model) ---\n",
        "    def save_checkpoint(self, epoch, is_best=False):\n",
        "        os.makedirs(self.hp.checkpoint_dir, exist_ok=True)\n",
        "        filename = os.path.join(self.hp.checkpoint_dir, f\"cRNN_checkpoint_epoch_{epoch}.pth\")\n",
        "        save_content = {\n",
        "            'epoch': epoch,\n",
        "            'decoder_state_dict': self.decoder.state_dict(), # Only decoder state\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'best_val_loss': self.best_val_loss,\n",
        "            'hp': vars(self.hp)\n",
        "        }\n",
        "        torch.save(save_content, filename)\n",
        "        print(f\"Checkpoint saved to {filename}\")\n",
        "        if is_best:\n",
        "            best_filename = os.path.join(self.hp.checkpoint_dir, \"cRNN_best_model.pth\")\n",
        "            torch.save(save_content, best_filename)\n",
        "            print(f\"*** Best validation model saved to {best_filename} ***\")\n",
        "\n",
        "    def load_checkpoint(self, filename=\"cRNN_best_model.pth\"):\n",
        "        filepath = os.path.join(self.hp.checkpoint_dir, filename)\n",
        "        if not os.path.exists(filepath):\n",
        "             print(f\"Checkpoint file not found: {filepath}. Training from scratch.\")\n",
        "             return 0\n",
        "        try:\n",
        "            checkpoint = torch.load(filepath, map_location=self.device)\n",
        "            self.decoder.load_state_dict(checkpoint['decoder_state_dict']) # Load decoder state\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            if 'scheduler_state_dict' in checkpoint:\n",
        "                 self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "            self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "            start_epoch = checkpoint['epoch'] + 1\n",
        "            print(f\"Loaded checkpoint '{filepath}' (trained up to epoch {checkpoint['epoch']})\")\n",
        "            print(f\"Best validation loss was: {self.best_val_loss}\")\n",
        "            return start_epoch\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint {filepath}: {e}. Training from scratch.\")\n",
        "            return 0\n",
        "\n",
        "    # --- Conditional Generation & Sampling (Modified for Autoregressive Decoder) ---\n",
        "    def conditional_generation(self, epoch, class_index, Nmax, data_scale_factor, temp=None):\n",
        "        self.decoder.eval() # Set decoder to eval mode\n",
        "        if temp is None: temp = self.hp.temperature\n",
        "\n",
        "        original_temp = self.hp.temperature\n",
        "        self.hp.temperature = temp # Set temp for sampling method\n",
        "\n",
        "        # Get target class embedding\n",
        "        class_indices = torch.tensor([class_index], dtype=torch.long, device=self.device)\n",
        "        embedded = self.decoder.embedding(class_indices) # Shape: (1, EmbSize)\n",
        "\n",
        "        # Initialize hidden state based on class embedding\n",
        "        hidden_cell = None # Let the forward pass initialize it\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Start with SOS token\n",
        "            s = torch.tensor([0, 0, 1, 0, 0], device=self.device, dtype=torch.float32).view(1, 1, 5) # Shape: (1, 1, 5)\n",
        "            seq_x, seq_y, seq_z = [], [], []\n",
        "\n",
        "            for i in range(Nmax):\n",
        "                # Prepare input for this step: last stroke 's' + class embedding\n",
        "                embedded_step = embedded.unsqueeze(0) # Shape: (1, 1, EmbSize)\n",
        "                decoder_input_step = torch.cat([s, embedded_step], dim=2) # Shape: (1, 1, 5 + EmbSize)\n",
        "\n",
        "                # Pass *single step* through decoder, using previous hidden state\n",
        "                self.pi, self.mu_x, self.mu_y, self.sigma_x, self.sigma_y, \\\n",
        "                self.rho_xy, self.q, hidden, cell = \\\n",
        "                    self.decoder(decoder_input_step, class_indices, hidden_cell) # Pass indices for potential re-init if needed (though shouldn't happen here)\n",
        "                hidden_cell = (hidden, cell) # Update hidden state for next step\n",
        "\n",
        "                # Sample the *next* stroke 's' based on the output distributions\n",
        "                # Ensure sample_next_state uses self.pi etc directly\n",
        "                s_next, dx, dy, pen_down, end_drawing = self.sample_next_state()\n",
        "\n",
        "                # Store denormalized stroke\n",
        "                seq_x.append(dx * data_scale_factor)\n",
        "                seq_y.append(dy * data_scale_factor)\n",
        "                seq_z.append(pen_down) # Store pen state\n",
        "\n",
        "                # Update 's' for the next iteration\n",
        "                s = s_next\n",
        "\n",
        "                if end_drawing:\n",
        "                    print(f\"Generated sequence length: {i+1}\")\n",
        "                    break\n",
        "\n",
        "        self.hp.temperature = original_temp # Restore original temp\n",
        "\n",
        "        if seq_x:\n",
        "            x_sample = np.cumsum(seq_x, 0)\n",
        "            y_sample = np.cumsum(seq_y, 0)\n",
        "            z_sample = np.array(seq_z)\n",
        "            sequence = np.stack([x_sample, y_sample, z_sample]).T\n",
        "            class_name = hp.classes[class_index]\n",
        "            img_name = f\"cRNN_epoch_{epoch}_class_{class_name}_temp_{temp:.2f}\"\n",
        "            make_image(sequence, img_name)\n",
        "\n",
        "    # sample_next_state: Needs access to self.pi, self.mu_x, etc. from the last decoder step\n",
        "    # It can be identical to the SketchRNN version as it only depends on the *output parameters*\n",
        "    def sample_next_state(self):\n",
        "        def adjust_temp(pi_pdf, temp):\n",
        "            pi_pdf = np.log(pi_pdf + 1e-8) / temp\n",
        "            pi_pdf -= np.max(pi_pdf)\n",
        "            pi_pdf = np.exp(pi_pdf)\n",
        "            pi_pdf /= np.sum(pi_pdf)\n",
        "            return pi_pdf\n",
        "\n",
        "        # Parameters (pi, mu_x, etc.) should be stored as attributes from the last forward pass\n",
        "        pi_cpu = self.pi.data[0, 0, :].cpu().numpy()\n",
        "        q_cpu = self.q.data[0, 0, :].cpu().numpy()\n",
        "\n",
        "        pi_adj = adjust_temp(pi_cpu, self.hp.temperature)\n",
        "        pi_idx = np.random.choice(self.hp.M, p=pi_adj)\n",
        "\n",
        "        q_adj = adjust_temp(q_cpu, self.hp.temperature)\n",
        "        q_idx = np.random.choice(3, p=q_adj)\n",
        "\n",
        "        mu_x = self.mu_x.data[0, 0, pi_idx].cpu().numpy()\n",
        "        mu_y = self.mu_y.data[0, 0, pi_idx].cpu().numpy()\n",
        "        sigma_x = self.sigma_x.data[0, 0, pi_idx].cpu().numpy()\n",
        "        sigma_y = self.sigma_y.data[0, 0, pi_idx].cpu().numpy()\n",
        "        rho_xy = self.rho_xy.data[0, 0, pi_idx].cpu().numpy()\n",
        "\n",
        "        dx, dy = sample_bivariate_normal(mu_x, mu_y, sigma_x, sigma_y, rho_xy, self.hp.temperature, greedy=False)\n",
        "\n",
        "        next_state = torch.zeros(5, device=self.device)\n",
        "        next_state[0] = dx\n",
        "        next_state[1] = dy\n",
        "        next_state[q_idx + 2] = 1\n",
        "\n",
        "        pen_down = (q_idx == 0)\n",
        "        end_drawing = (q_idx == 2)\n",
        "\n",
        "        return next_state.view(1, 1, 5), dx, dy, pen_down, end_drawing"
      ],
      "metadata": {
        "id": "W7OB15hTk8se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Miscellaneous Functions"
      ],
      "metadata": {
        "id": "ZaHTxSyzXPYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_bivariate_normal(mu_x, mu_y, sigma_x, sigma_y, rho_xy, temp, greedy=False):\n",
        "    if greedy: return mu_x, mu_y\n",
        "    sigma_x *= np.sqrt(temp); sigma_y *= np.sqrt(temp)\n",
        "    sigma_x = max(sigma_x, 1e-4); sigma_y = max(sigma_y, 1e-4); rho_xy = np.clip(rho_xy, -1 + 1e-4, 1 - 1e-4)\n",
        "    mean = [mu_x, mu_y]; cov = [[sigma_x * sigma_x, rho_xy * sigma_x * sigma_y], [rho_xy * sigma_x * sigma_y, sigma_y * sigma_y]]\n",
        "    try: x = np.random.multivariate_normal(mean, cov, 1); return x[0][0], x[0][1]\n",
        "    except np.linalg.LinAlgError as e: print(f\"Warning: LinAlgError in multivariate_normal: {e}. Returning mean.\"); return mu_x, mu_y"
      ],
      "metadata": {
        "id": "jg3xXtUklN1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_image(sequence, name='output_sketch'):\n",
        "    if sequence is None or len(sequence) == 0 or sequence.shape[1] < 3: print(\"Warning: Cannot make image from empty or invalid sequence.\"); return\n",
        "    pen_lift_indices = np.where(sequence[:, 2] < 0.5)[0]\n",
        "    strokes = np.split(sequence, pen_lift_indices + 1)\n",
        "    fig = plt.figure(figsize=(6, 6)); ax1 = fig.add_subplot(111); ax1.set_aspect('equal', adjustable='box')\n",
        "    cumulative_pos = np.zeros(2)\n",
        "    for s in strokes:\n",
        "        if len(s) == 0: continue\n",
        "        points = np.cumsum(s[:, :2], axis=0)\n",
        "        plt.plot(points[:, 0] + cumulative_pos[0], -(points[:, 1] + cumulative_pos[1]), 'k-')\n",
        "        if len(points) > 0: cumulative_pos += points[-1]\n",
        "    plt.axis('off'); plt.gca().invert_yaxis()\n",
        "    save_filename = name + '.png'\n",
        "    try: plt.savefig(save_filename, bbox_inches='tight', pad_inches=0.1); print(f\"Image saved to {save_filename}\")\n",
        "    except Exception as e: print(f\"Error saving image {save_filename}: {e}\")\n",
        "    plt.close(fig)"
      ],
      "metadata": {
        "id": "sFU2M88mlQQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model"
      ],
      "metadata": {
        "id": "A4qUO1v5XlC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(num_classes=num_classes, hp=hp, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0huK9Srl5SX",
        "outputId": "76252f56-3deb-4cba-dc42-862e3528bf6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter(hp.log_dir)"
      ],
      "metadata": {
        "id": "U3CvvHG2nKhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(hp.num_epochs):\n",
        "    # --- Training Step ---\n",
        "    train_loss = model.train_epoch(epoch, train_data, Nmax)\n",
        "\n",
        "    # --- Validation Step ---\n",
        "    if (epoch + 1) % hp.epochs_til_validation == 0 or epoch == hp.num_epochs - 1:\n",
        "        val_loss = model.evaluate(validation_data, Nmax)\n",
        "        model.scheduler.step(val_loss) # Adjust LR based on validation loss\n",
        "\n",
        "        is_best = val_loss < model.best_val_loss\n",
        "        if is_best:\n",
        "            model.best_val_loss = val_loss\n",
        "            print(f\"*** New best validation loss: {val_loss:.4f} at epoch {epoch} ***\")\n",
        "\n",
        "        # --- Checkpointing ---\n",
        "        if (epoch + 1) % hp.epochs_til_checkpoint == 0 or is_best:\n",
        "             model.save_checkpoint(epoch, is_best=is_best)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uk2YYllBl79u",
        "outputId": "5f9c09d4-1ea0-4352-9dfd-091f3582a005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Train Loss: 0.8402\n",
            "Epoch 1 | Train Loss: 0.5056\n",
            "Epoch 2 | Train Loss: 0.4167\n",
            "Epoch 3 | Train Loss: 0.3660\n",
            "Epoch 4 | Train Loss: 0.3296\n",
            "Epoch 5 | Train Loss: 0.3018\n",
            "Epoch 6 | Train Loss: 0.2789\n",
            "Epoch 7 | Train Loss: 0.2597\n",
            "Epoch 8 | Train Loss: 0.2442\n",
            "Epoch 9 | Train Loss: 0.2325\n",
            "Epoch 10 | Train Loss: 0.2235\n",
            "Epoch 11 | Train Loss: 0.2115\n",
            "Epoch 12 | Train Loss: 0.2026\n",
            "Epoch 13 | Train Loss: 0.1964\n",
            "Epoch 14 | Train Loss: 0.1876\n",
            "Epoch 15 | Train Loss: 0.1802\n",
            "Epoch 16 | Train Loss: 0.1731\n",
            "Epoch 17 | Train Loss: 0.1659\n",
            "Epoch 18 | Train Loss: 0.1629\n",
            "Epoch 19 | Train Loss: 0.1563\n",
            "--- Validation | Loss: 0.2124 ---\n",
            "*** New best validation loss: 0.2124 at epoch 19 ***\n",
            "Checkpoint saved to /content/cRNN_checkpoints/cRNN_checkpoint_epoch_19.pth\n",
            "*** Best validation model saved to /content/cRNN_checkpoints/cRNN_best_model.pth ***\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_success = model.load_checkpoint(\"cRNN_best_model.pth\")"
      ],
      "metadata": {
        "id": "Thgd6gEKmKe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8b3cdd4-eefe-43b3-aec6-a525f5c82720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint '/content/cRNN_checkpoints/cRNN_best_model.pth' (trained up to epoch 19)\n",
            "Best validation loss was: 0.2123752439948084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sketch Generation Function"
      ],
      "metadata": {
        "id": "ltiqGTrtXI--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Animation Function (Conditional RNN version - WITH VERTICAL FLIP POST-PLOT) ---\n",
        "def generate_drawing_animation_cRNN(model, class_index, Nmax, data_scale_factor, num_frames=None, save_path='cRNN_drawing_animation_flipped.gif', temperature=0.4): # Changed default save_path\n",
        "    \"\"\"\n",
        "    Generates animation for the Conditional RNN model.\n",
        "    The final GIF output will be vertically flipped.\n",
        "    \"\"\"\n",
        "\n",
        "    model.decoder.eval() # Set decoder to eval mode\n",
        "\n",
        "    # Use model's hp, but override temperature for this generation\n",
        "    hp = model.hp # Get hp from the model instance\n",
        "    original_temp = hp.temperature\n",
        "    hp.temperature = temperature # Set generation temp\n",
        "\n",
        "    # Get class embedding and init hidden state\n",
        "    class_indices = torch.tensor([class_index], dtype=torch.long, device=model.device)\n",
        "    embedded = model.decoder.embedding(class_indices)\n",
        "    hidden_cell = None # Let decoder initialize\n",
        "\n",
        "    seq_x, seq_y, seq_z = [], [], []\n",
        "    s = torch.tensor([0, 0, 1, 0, 0], device=model.device, dtype=torch.float32).view(1, 1, 5) # SOS token\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(Nmax): # Generate up to Nmax steps\n",
        "            embedded_step = embedded.unsqueeze(0) # Shape: (1, 1, EmbSize)\n",
        "            decoder_input_step = torch.cat([s, embedded_step], dim=2) # Shape: (1, 1, 5 + EmbSize)\n",
        "\n",
        "            # Single step forward\n",
        "            model.pi, model.mu_x, model.mu_y, model.sigma_x, model.sigma_y, \\\n",
        "            model.rho_xy, model.q, hidden, cell = \\\n",
        "                model.decoder(decoder_input_step, class_indices, hidden_cell)\n",
        "            hidden_cell = (hidden, cell) # Update for next step\n",
        "\n",
        "            # Sample next state\n",
        "            s_next, dx, dy, pen_down, end_drawing = model.sample_next_state()\n",
        "\n",
        "            # Store denormalized stroke\n",
        "            seq_x.append(dx * data_scale_factor)\n",
        "            seq_y.append(dy * data_scale_factor)\n",
        "            seq_z.append(pen_down) # Store pen state (True if down)\n",
        "\n",
        "            s = s_next # Update s for the next loop\n",
        "\n",
        "            if end_drawing:\n",
        "                # print(f\"Animation: Drawing completed in {i+1} steps (End of Drawing signal).\")\n",
        "                break\n",
        "        # else:\n",
        "             # print(f\"Animation: Reached Nmax ({Nmax}) steps without End signal.\")\n",
        "\n",
        "    hp.temperature = original_temp # Restore original temp\n",
        "\n",
        "    if not seq_x:\n",
        "        print(\"Animation failed: No sequence generated.\")\n",
        "        return None\n",
        "\n",
        "    # --- Frame Generation and GIF Saving ---\n",
        "    x_sample = np.cumsum(seq_x, 0)\n",
        "    y_sample = np.cumsum(seq_y, 0) # Keep Y positive for plotting consistency\n",
        "    z_sample = np.array(seq_z) # Boolean array (True if pen down)\n",
        "\n",
        "    total_generated_frames = len(x_sample)\n",
        "    frame_indices = np.arange(total_generated_frames)\n",
        "    if num_frames is not None and num_frames > 0 and num_frames < total_generated_frames:\n",
        "        frame_indices = np.linspace(0, total_generated_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    output_frames = len(frame_indices)\n",
        "    frames = []\n",
        "    print(f\"Creating vertically flipped animation '{save_path}' with {output_frames} frames...\")\n",
        "\n",
        "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.set_aspect('equal', adjustable='box')\n",
        "\n",
        "    # Calculate plot limits (using positive Y)\n",
        "    if len(x_sample) > 0:\n",
        "        x_min, x_max = np.min(x_sample), np.max(x_sample)\n",
        "        y_min, y_max = np.min(y_sample), np.max(y_sample)\n",
        "        x_range = x_max - x_min\n",
        "        y_range = y_max - y_min\n",
        "        x_padding = x_range * 0.1 if x_range > 1e-4 else 1.0\n",
        "        y_padding = y_range * 0.1 if y_range > 1e-4 else 1.0\n",
        "        plot_xlim = (x_min - x_padding, x_max + x_padding)\n",
        "        plot_ylim = (y_min - y_padding, y_max + y_padding)\n",
        "    else:\n",
        "        plot_xlim = (-1, 1); plot_ylim = (-1, 1)\n",
        "\n",
        "    for frame_num, idx in enumerate(frame_indices):\n",
        "        current_idx_in_sequence = idx\n",
        "        ax.clear()\n",
        "        current_x = x_sample[:current_idx_in_sequence+1]\n",
        "        current_y = y_sample[:current_idx_in_sequence+1]\n",
        "        current_z = z_sample[:current_idx_in_sequence+1]\n",
        "\n",
        "        lift_indices = np.where(~current_z)[0]\n",
        "        start_plot_idx = 0\n",
        "        for lift_idx in lift_indices:\n",
        "            segment_x = current_x[start_plot_idx : lift_idx+1]\n",
        "            segment_y = current_y[start_plot_idx : lift_idx+1]\n",
        "            if len(segment_x) > 1:\n",
        "                 ax.plot(segment_x, segment_y, 'k-', linewidth=1.5) # Plot positive Y\n",
        "            elif len(segment_x) == 1 :\n",
        "                  ax.plot(segment_x[0], segment_y[0], 'k.', markersize=2)\n",
        "            start_plot_idx = lift_idx + 1\n",
        "\n",
        "        if start_plot_idx < len(current_x):\n",
        "             segment_x = current_x[start_plot_idx:]\n",
        "             segment_y = current_y[start_plot_idx:]\n",
        "             if len(segment_x) > 1:\n",
        "                  ax.plot(segment_x, segment_y, 'k-', linewidth=1.5) # Plot positive Y\n",
        "             elif len(segment_x) == 1:\n",
        "                  ax.plot(segment_x[0], segment_y[0], 'k.', markersize=2)\n",
        "\n",
        "        # --- Set up plot appearance (Keep Y increasing upwards for plot consistency) ---\n",
        "        ax.set_xlim(plot_xlim)\n",
        "        ax.set_ylim(plot_ylim)\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f\"Class: {hp.classes[class_index]} | Temp: {temperature:.2f} | Frame {frame_num+1}/{output_frames}\")\n",
        "        # NO axis inversion here\n",
        "\n",
        "        # --- Save frame to buffer ---\n",
        "        canvas = FigureCanvas(fig)\n",
        "        canvas.draw()\n",
        "        buf = io.BytesIO()\n",
        "        fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0.1)\n",
        "        buf.seek(0)\n",
        "        img = Image.open(buf).convert('RGB')\n",
        "\n",
        "        # <<< --- VERTICAL FLIP ADDED HERE --- >>>\n",
        "        img_flipped = ImageOps.flip(img)\n",
        "        frames.append(img_flipped) # Append the flipped image to the list\n",
        "        # <<< --- END OF FLIP --- >>>\n",
        "\n",
        "        buf.close()\n",
        "\n",
        "    plt.close(fig) # Close the figure after generating all frames\n",
        "\n",
        "    if not frames:\n",
        "         print(\"No frames generated for GIF.\")\n",
        "         return None\n",
        "\n",
        "    # --- Save GIF ---\n",
        "    duration_ms = max(20, 1000 // 30)\n",
        "    try:\n",
        "        frames[0].save(\n",
        "            save_path,\n",
        "            format='GIF',\n",
        "            append_images=frames[1:],\n",
        "            save_all=True,\n",
        "            duration=duration_ms,\n",
        "            loop=0\n",
        "        )\n",
        "        print(f\"Animation saved to {save_path}\")\n",
        "        return save_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving GIF {save_path}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "d0ZZkXB-9V9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_success = model.load_checkpoint(\"cRNN_best_model.pth\")\n",
        "if load_success:\n",
        "     print(\"Generating final animations using the best model...\")\n",
        "     for i, class_name in enumerate(hp.classes):\n",
        "         print(f\"--- Generating animation for: {class_name} ---\")\n",
        "         output_gif_path = f\"cRNN_{class_name}_final_animation_FLIPPED.gif\" # New filename\n",
        "         generate_drawing_animation_cRNN( # Call the corrected function\n",
        "             model=model,\n",
        "             class_index=i,\n",
        "             Nmax=Nmax,\n",
        "             data_scale_factor=data_scale_factor,\n",
        "             num_frames=200,\n",
        "             save_path=output_gif_path,\n",
        "             temperature=0.35\n",
        "         )\n",
        "else:\n",
        "     print(\"Could not load model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm2jpT9o7chh",
        "outputId": "7d42788f-8c4a-4616-a9a9-5af3ac93efad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint '/content/cRNN_checkpoints/cRNN_best_model.pth' (trained up to epoch 19)\n",
            "Best validation loss was: 0.2123752439948084\n",
            "Generating final animations using the best model...\n",
            "--- Generating animation for: cat ---\n",
            "Creating vertically flipped animation 'cRNN_cat_final_animation_FLIPPED.gif' with 52 frames...\n",
            "Animation saved to cRNN_cat_final_animation_FLIPPED.gif\n",
            "--- Generating animation for: apple ---\n",
            "Creating vertically flipped animation 'cRNN_apple_final_animation_FLIPPED.gif' with 99 frames...\n",
            "Animation saved to cRNN_apple_final_animation_FLIPPED.gif\n",
            "--- Generating animation for: airplane ---\n",
            "Creating vertically flipped animation 'cRNN_airplane_final_animation_FLIPPED.gif' with 99 frames...\n",
            "Animation saved to cRNN_airplane_final_animation_FLIPPED.gif\n",
            "--- Generating animation for: candle ---\n",
            "Creating vertically flipped animation 'cRNN_candle_final_animation_FLIPPED.gif' with 29 frames...\n",
            "Animation saved to cRNN_candle_final_animation_FLIPPED.gif\n",
            "--- Generating animation for: alarm clock ---\n",
            "Creating vertically flipped animation 'cRNN_alarm clock_final_animation_FLIPPED.gif' with 74 frames...\n",
            "Animation saved to cRNN_alarm clock_final_animation_FLIPPED.gif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus Task-1"
      ],
      "metadata": {
        "id": "xmRERJz4WS5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_drawing_animation_cRNN_bonus(model, class_indices, Nmax, data_scale_factor, num_frames=None, save_path='cRNN_parallel_animation_flipped.gif', temperature=0.4):\n",
        "    \"\"\"\n",
        "    Generates animation with multiple sketches drawn simultaneously side-by-side.\n",
        "    Maintains individual vertical flip for each subplot.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import PIL\n",
        "    from PIL import Image, ImageOps\n",
        "    import io\n",
        "    from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "\n",
        "    model.decoder.eval()\n",
        "    hp = model.hp\n",
        "    original_temp = hp.temperature\n",
        "    hp.temperature = temperature\n",
        "\n",
        "    # Pre-generate all sequences first\n",
        "    all_sequences = []\n",
        "    max_length = 0\n",
        "    for class_index in class_indices:\n",
        "        # Generate stroke sequence for this class\n",
        "        class_indices_tensor = torch.tensor([class_index], dtype=torch.long, device=model.device)\n",
        "        embedded = model.decoder.embedding(class_indices_tensor)\n",
        "        hidden_cell = None\n",
        "        s = torch.tensor([0, 0, 1, 0, 0], device=model.device, dtype=torch.float32).view(1, 1, 5)\n",
        "\n",
        "        seq_x, seq_y, seq_z = [], [], []\n",
        "        with torch.no_grad():\n",
        "            for i in range(Nmax):\n",
        "                embedded_step = embedded.unsqueeze(0)\n",
        "                decoder_input_step = torch.cat([s, embedded_step], dim=2)\n",
        "\n",
        "                model.pi, model.mu_x, model.mu_y, model.sigma_x, model.sigma_y, \\\n",
        "                model.rho_xy, model.q, hidden, cell = \\\n",
        "                    model.decoder(decoder_input_step, class_indices_tensor, hidden_cell)\n",
        "                hidden_cell = (hidden, cell)\n",
        "\n",
        "                s_next, dx, dy, pen_down, end_drawing = model.sample_next_state()\n",
        "\n",
        "                seq_x.append(dx * data_scale_factor)\n",
        "                seq_y.append(dy * data_scale_factor)\n",
        "                seq_z.append(pen_down)\n",
        "                s = s_next\n",
        "\n",
        "                if end_drawing:\n",
        "                    break\n",
        "\n",
        "        x_sample = np.cumsum(seq_x, 0)\n",
        "        y_sample = np.cumsum(seq_y, 0)\n",
        "        z_sample = np.array(seq_z)\n",
        "\n",
        "        all_sequences.append({\n",
        "            'x': x_sample,\n",
        "            'y': y_sample,\n",
        "            'z': z_sample,\n",
        "            'class_name': hp.classes[class_index],\n",
        "            'length': len(x_sample)\n",
        "        })\n",
        "\n",
        "        if len(x_sample) > max_length:\n",
        "            max_length = len(x_sample)\n",
        "\n",
        "    if max_length == 0:\n",
        "        print(\"No valid sequences generated for any class\")\n",
        "        return None\n",
        "\n",
        "    # Determine frame indices based on longest sequence\n",
        "    frame_indices = np.arange(max_length)\n",
        "    if num_frames is not None and num_frames > 0 and num_frames < max_length:\n",
        "        frame_indices = np.linspace(0, max_length-1, num_frames, dtype=int)\n",
        "\n",
        "    # Setup subplot grid\n",
        "    n_classes = len(class_indices)\n",
        "    n_cols = int(np.ceil(np.sqrt(n_classes)))\n",
        "    n_rows = int(np.ceil(n_classes / n_cols))\n",
        "\n",
        "    fig = plt.figure(figsize=(6 * n_cols, 6 * n_rows), dpi=100)\n",
        "    axes = [fig.add_subplot(n_rows, n_cols, i+1) for i in range(n_classes)]\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.set_aspect('equal', adjustable='box')\n",
        "        ax.axis('off')\n",
        "\n",
        "    frames = []\n",
        "    print(f\"Creating parallel animation with {len(frame_indices)} frames...\")\n",
        "\n",
        "    for frame_idx in frame_indices:\n",
        "        for ax in axes:\n",
        "            ax.clear()\n",
        "\n",
        "        for idx, (seq, ax) in enumerate(zip(all_sequences, axes)):\n",
        "            current_idx = min(frame_idx, seq['length']-1)  # Handle finished sequences\n",
        "            current_x = seq['x'][:current_idx+1]\n",
        "            current_y = seq['y'][:current_idx+1]\n",
        "            current_z = seq['z'][:current_idx+1]\n",
        "\n",
        "            # Plotting logic for each subplot\n",
        "            lift_indices = np.where(~current_z)[0]\n",
        "            start_plot_idx = 0\n",
        "            for lift_idx in lift_indices:\n",
        "                segment_x = current_x[start_plot_idx : lift_idx+1]\n",
        "                segment_y = current_y[start_plot_idx : lift_idx+1]\n",
        "                if len(segment_x) > 1:\n",
        "                    ax.plot(segment_x, segment_y, 'k-', linewidth=1.5)\n",
        "                start_plot_idx = lift_idx + 1\n",
        "\n",
        "            if start_plot_idx < len(current_x):\n",
        "                segment_x = current_x[start_plot_idx:]\n",
        "                segment_y = current_y[start_plot_idx:]\n",
        "                if len(segment_x) > 1:\n",
        "                    ax.plot(segment_x, segment_y, 'k-', linewidth=1.5)\n",
        "\n",
        "            # Set individual plot limits\n",
        "            x_pad = (np.max(seq['x']) - np.min(seq['x'])) * 0.1 if len(seq['x']) > 0 else 1.0\n",
        "            y_pad = (np.max(seq['y']) - np.min(seq['y'])) * 0.1 if len(seq['y']) > 0 else 1.0\n",
        "            ax.set_xlim(np.min(seq['x'])-x_pad if len(seq['x']) > 0 else -1,\n",
        "                       np.max(seq['x'])+x_pad if len(seq['x']) > 0 else 1)\n",
        "            ax.set_ylim(np.min(seq['y'])-y_pad if len(seq['y']) > 0 else -1,\n",
        "                       np.max(seq['y'])+y_pad if len(seq['y']) > 0 else 1)\n",
        "\n",
        "            ax.set_title(f\"{seq['class_name']}\\nStep: {current_idx+1}/{seq['length']}\")\n",
        "\n",
        "        # Save combined frame\n",
        "        canvas = FigureCanvas(fig)\n",
        "        canvas.draw()\n",
        "        buf = io.BytesIO()\n",
        "        fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0.1)\n",
        "        buf.seek(0)\n",
        "        img = ImageOps.flip(Image.open(buf).convert('RGB'))\n",
        "        frames.append(img)\n",
        "        buf.close()\n",
        "\n",
        "    plt.close(fig)\n",
        "    hp.temperature = original_temp\n",
        "\n",
        "    if frames:\n",
        "        duration = max(20, 1000 // 30)\n",
        "        frames[0].save(\n",
        "            save_path,\n",
        "            format='GIF',\n",
        "            append_images=frames[1:],\n",
        "            save_all=True,\n",
        "            duration=duration,\n",
        "            loop=0\n",
        "        )\n",
        "        print(f\"Parallel animation saved to {save_path}\")\n",
        "        return save_path\n",
        "    print(\"No frames generated for GIF.\")\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "TO_I7E0DA1t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_drawing_animation_cRNN_bonus(\n",
        "    model=model,\n",
        "    class_indices=[0, 1, 2, 3],  # 4 classes\n",
        "    Nmax=Nmax,\n",
        "    data_scale_factor=data_scale_factor,\n",
        "    num_frames=300,\n",
        "    save_path=\"four_classes_parallel.gif\",\n",
        "    temperature=0.4\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "NSolQSn1HDaJ",
        "outputId": "03fa1d99-0c5c-49e4-8bcd-fcc178ff2163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating parallel animation with 129 frames...\n",
            "Parallel animation saved to four_classes_parallel.gif\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'four_classes_parallel.gif'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus Task-2"
      ],
      "metadata": {
        "id": "xtbRl5Xe0aU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _generate_single_object_sequence(model, class_index, Nmax, data_scale_factor, temperature):\n",
        "    \"\"\"Generates the stroke sequence (dx, dy, pen_down) for a single object.\"\"\"\n",
        "    model.decoder.eval()  # Set decoder to eval mode\n",
        "\n",
        "    # Use model's hp, but override temperature for this generation\n",
        "    hp = model.hp\n",
        "    original_temp = hp.temperature\n",
        "    hp.temperature = temperature  # Set generation temp\n",
        "\n",
        "    # Get class embedding and init hidden state\n",
        "    class_indices = torch.tensor([class_index], dtype=torch.long, device=model.device)\n",
        "    embedded = model.decoder.embedding(class_indices)\n",
        "    hidden_cell = None  # Let decoder initialize\n",
        "\n",
        "    seq_x_rel, seq_y_rel, seq_z_pen_down = [], [], [] # Relative dx, dy\n",
        "    # Start with SOS token\n",
        "    s = torch.tensor([0, 0, 1, 0, 0], device=model.device, dtype=torch.float32).view(1, 1, 5)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(Nmax):  # Generate up to Nmax steps\n",
        "            embedded_step = embedded.unsqueeze(0)  # Shape: (1, 1, EmbSize)\n",
        "            decoder_input_step = torch.cat([s, embedded_step], dim=2) # Shape: (1, 1, 5 + EmbSize)\n",
        "\n",
        "            # Single step forward\n",
        "            model.pi, model.mu_x, model.mu_y, model.sigma_x, model.sigma_y, \\\n",
        "            model.rho_xy, model.q, hidden, cell = \\\n",
        "                model.decoder(decoder_input_step, class_indices, hidden_cell)\n",
        "            hidden_cell = (hidden, cell)  # Update for next step\n",
        "\n",
        "            # Sample next state (uses model attributes like self.pi set above)\n",
        "            s_next, dx, dy, pen_down, end_drawing = model.sample_next_state()\n",
        "\n",
        "            # Store denormalized *relative* stroke\n",
        "            seq_x_rel.append(dx * data_scale_factor)\n",
        "            seq_y_rel.append(dy * data_scale_factor)\n",
        "            seq_z_pen_down.append(pen_down)  # Store pen state (True if down)\n",
        "\n",
        "            s = s_next  # Update s for the next loop\n",
        "\n",
        "            if end_drawing:\n",
        "                # print(f\"Object {hp.classes[class_index]} completed in {i+1} steps.\")\n",
        "                break\n",
        "        # else:\n",
        "            # print(f\"Object {hp.classes[class_index]} reached Nmax ({Nmax}) steps.\")\n",
        "\n",
        "    hp.temperature = original_temp  # Restore original temp\n",
        "\n",
        "    return np.array(seq_x_rel), np.array(seq_y_rel), np.array(seq_z_pen_down)\n"
      ],
      "metadata": {
        "id": "5HVKsS_U0T5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_scene_animation(model, class_indices, Nmax, data_scale_factor,\n",
        "                             num_frames=None, save_path='cRNN_scene_animation_flipped.gif',\n",
        "                             temperature=0.4, spacing_factor=1.5, initial_offset=(0,0)):\n",
        "    \"\"\"\n",
        "    Generates a scene animation with multiple objects drawn sequentially.\n",
        "    Applies vertical flip to the final output.\n",
        "\n",
        "    Args:\n",
        "        model (Model): The loaded trained model instance.\n",
        "        class_indices (list[int]): List of class indices for the objects to draw.\n",
        "        Nmax (int): Max sequence length *per object*.\n",
        "        data_scale_factor (float): Normalization scale factor.\n",
        "        num_frames (int, optional): Total frames for the entire animation. Defaults to dynamic.\n",
        "        save_path (str): Path to save the output GIF.\n",
        "        temperature (float): Sampling temperature.\n",
        "        spacing_factor (float): Multiplier for spacing based on previous object width.\n",
        "        initial_offset (tuple): Starting (x, y) for the first object.\n",
        "    \"\"\"\n",
        "    print(f\"Starting scene generation for classes: {[model.hp.classes[i] for i in class_indices]}\")\n",
        "    model.decoder.eval()\n",
        "    hp = model.hp\n",
        "\n",
        "    all_abs_x = []      # Stores absolute X coordinates for the entire scene\n",
        "    all_abs_y = []      # Stores absolute Y coordinates for the entire scene\n",
        "    all_pen_down = []   # Stores pen state (True=down) for the entire scene\n",
        "    object_start_indices = [0] # Index in the combined list where each object starts\n",
        "\n",
        "    current_offset_x, current_offset_y = initial_offset\n",
        "\n",
        "    for i, class_idx in enumerate(class_indices):\n",
        "        print(f\"  Generating object {i+1}/{len(class_indices)}: {hp.classes[class_idx]}\")\n",
        "        seq_x_rel, seq_y_rel, seq_z = _generate_single_object_sequence(\n",
        "            model, class_idx, Nmax, data_scale_factor, temperature\n",
        "        )\n",
        "\n",
        "        if len(seq_x_rel) == 0:\n",
        "            print(f\"  Warning: No sequence generated for object {hp.classes[class_idx]}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Calculate absolute coordinates for this object\n",
        "        abs_x = np.cumsum(seq_x_rel) + current_offset_x\n",
        "        abs_y = np.cumsum(seq_y_rel) + current_offset_y\n",
        "\n",
        "        # Append to the master lists\n",
        "        all_abs_x.extend(abs_x.tolist())\n",
        "        all_abs_y.extend(abs_y.tolist())\n",
        "        all_pen_down.extend(seq_z.tolist())\n",
        "        object_start_indices.append(len(all_abs_x)) # Mark end of this object / start of next\n",
        "\n",
        "        # --- Calculate offset for the next object (simple horizontal placement) ---\n",
        "        if len(abs_x) > 0:\n",
        "            min_x, max_x = np.min(abs_x), np.max(abs_x)\n",
        "            width = max_x - min_x\n",
        "            # Update offset to be to the right of the current object\n",
        "            current_offset_x = max_x + width * (spacing_factor - 1.0)\n",
        "            # You could add logic here to move to the next \"row\" if current_offset_x exceeds a limit\n",
        "            # current_offset_y = ...\n",
        "        print(f\"    Object {hp.classes[class_idx]} added. Next offset: ({current_offset_x:.1f}, {current_offset_y:.1f})\")\n",
        "\n",
        "\n",
        "    # --- Convert combined lists to numpy arrays ---\n",
        "    all_abs_x = np.array(all_abs_x)\n",
        "    all_abs_y = np.array(all_abs_y)\n",
        "    all_pen_down = np.array(all_pen_down) # Boolean array\n",
        "\n",
        "    total_scene_strokes = len(all_abs_x)\n",
        "    if total_scene_strokes == 0:\n",
        "        print(\"Animation failed: No strokes generated for the entire scene.\")\n",
        "        return None\n",
        "\n",
        "    # --- Frame Generation and GIF Saving ---\n",
        "    frame_indices = np.arange(total_scene_strokes)\n",
        "    if num_frames is not None and num_frames > 0 and num_frames < total_scene_strokes:\n",
        "        frame_indices = np.linspace(0, total_scene_strokes - 1, num_frames, dtype=int)\n",
        "    elif total_scene_strokes > 1000: # Limit default frames for very long sequences\n",
        "         frame_indices = np.linspace(0, total_scene_strokes - 1, 1000, dtype=int)\n",
        "\n",
        "\n",
        "    output_frames = len(frame_indices)\n",
        "    frames = []\n",
        "    print(f\"Creating vertically flipped scene animation '{save_path}' with {output_frames} frames...\")\n",
        "\n",
        "    fig = plt.figure(figsize=(8, 6), dpi=100) # Potentially wider figure for scene\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.set_aspect('equal', adjustable='box')\n",
        "\n",
        "    # Calculate plot limits for the entire scene\n",
        "    if total_scene_strokes > 0:\n",
        "        x_min, x_max = np.min(all_abs_x), np.max(all_abs_x)\n",
        "        y_min, y_max = np.min(all_abs_y), np.max(all_abs_y)\n",
        "        x_range = x_max - x_min\n",
        "        y_range = y_max - y_min\n",
        "        x_padding = x_range * 0.1 if x_range > 1e-4 else 5.0 # Add some padding\n",
        "        y_padding = y_range * 0.1 if y_range > 1e-4 else 5.0\n",
        "        plot_xlim = (x_min - x_padding, x_max + x_padding)\n",
        "        plot_ylim = (y_min - y_padding, y_max + y_padding)\n",
        "    else:\n",
        "        plot_xlim = (-10, 10); plot_ylim = (-10, 10)\n",
        "\n",
        "    # Find which object is being drawn at each frame index for title\n",
        "    current_object_idx = 0\n",
        "\n",
        "    for frame_num, master_idx in enumerate(frame_indices):\n",
        "        # Determine which object is being drawn\n",
        "        while current_object_idx + 1 < len(object_start_indices) and master_idx >= object_start_indices[current_object_idx+1]:\n",
        "            current_object_idx += 1\n",
        "        current_class_name = hp.classes[class_indices[current_object_idx]] if current_object_idx < len(class_indices) else \"Finished\"\n",
        "\n",
        "        ax.clear()\n",
        "        # Get coordinates and pen states up to the current master index\n",
        "        current_x = all_abs_x[:master_idx+1]\n",
        "        current_y = all_abs_y[:master_idx+1]\n",
        "        current_z_down = all_pen_down[:master_idx+1] # Pen is down (True) or up (False)\n",
        "\n",
        "        # Plot segments based on pen lifts (where current_z_down is False)\n",
        "        lift_indices = np.where(~current_z_down)[0] # Indices where pen is UP\n",
        "        start_plot_idx = 0\n",
        "        for lift_idx in lift_indices:\n",
        "            # Plot segment before the lift\n",
        "            segment_x = current_x[start_plot_idx : lift_idx+1]\n",
        "            segment_y = current_y[start_plot_idx : lift_idx+1]\n",
        "            if len(segment_x) > 1:\n",
        "                 ax.plot(segment_x, segment_y, 'k-', linewidth=1.5)\n",
        "            # elif len(segment_x) == 1 : # Draw single points if needed\n",
        "            #       ax.plot(segment_x[0], segment_y[0], 'k.', markersize=2)\n",
        "            start_plot_idx = lift_idx + 1 # Start next segment after the lift\n",
        "\n",
        "        # Plot the last segment (or the only segment if no lifts)\n",
        "        if start_plot_idx < len(current_x):\n",
        "             segment_x = current_x[start_plot_idx:]\n",
        "             segment_y = current_y[start_plot_idx:]\n",
        "             if len(segment_x) > 1:\n",
        "                  ax.plot(segment_x, segment_y, 'k-', linewidth=1.5)\n",
        "             # elif len(segment_x) == 1:\n",
        "             #      ax.plot(segment_x[0], segment_y[0], 'k.', markersize=2)\n",
        "\n",
        "        # --- Set up plot appearance (Keep Y increasing upwards for plot consistency) ---\n",
        "        ax.set_xlim(plot_xlim)\n",
        "        ax.set_ylim(plot_ylim)\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f\"Scene | Drawing: {current_class_name} | Temp: {temperature:.2f} | Frame {frame_num+1}/{output_frames}\")\n",
        "\n",
        "        # --- Save frame to buffer ---\n",
        "        canvas = FigureCanvas(fig)\n",
        "        canvas.draw()\n",
        "        buf = io.BytesIO()\n",
        "        fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0.1)\n",
        "        buf.seek(0)\n",
        "        img = Image.open(buf).convert('RGB')\n",
        "\n",
        "        # <<< --- VERTICAL FLIP ADDED HERE --- >>>\n",
        "        img_flipped = ImageOps.flip(img)\n",
        "        frames.append(img_flipped) # Append the flipped image\n",
        "        # <<< --- END OF FLIP --- >>>\n",
        "\n",
        "        buf.close()\n",
        "\n",
        "    plt.close(fig) # Close the figure after generating all frames\n",
        "\n",
        "    if not frames:\n",
        "         print(\"No frames generated for scene GIF.\")\n",
        "         return None\n",
        "\n",
        "    # --- Save GIF ---\n",
        "    duration_ms = max(20, 1000 // 30) # Aim for ~30 fps, min 20ms\n",
        "    try:\n",
        "        frames[0].save(\n",
        "            save_path,\n",
        "            format='GIF',\n",
        "            append_images=frames[1:],\n",
        "            save_all=True,\n",
        "            duration=duration_ms,\n",
        "            loop=0 # Loop indefinitely\n",
        "        )\n",
        "        print(f\"Scene animation saved to {save_path}\")\n",
        "        return save_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving scene GIF {save_path}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "83a9VQBUv5aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx = {name: i for i, name in enumerate(hp.classes)}\n",
        "scene_objects = ['cat', 'apple', 'airplane'] # Example scene"
      ],
      "metadata": {
        "id": "CsGvpDJozNBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  scene_indices = [class_to_idx[name] for name in scene_objects]\n",
        "except KeyError as e:\n",
        "  print(f\"Error: Class '{e}' not found in model's known classes: {hp.classes}\")\n",
        "  print(\"Cannot generate scene.\")\n",
        "  scene_indices = [] # Prevent further execution\n",
        "\n",
        "if scene_indices:\n",
        "  # 4. Generate the Scene Animation\n",
        "  output_gif_path = \"my_cool_scene_animation_FLIPPED.gif\"\n",
        "  generate_scene_animation(\n",
        "      model=model,\n",
        "      class_indices=scene_indices,\n",
        "      Nmax=hp.max_seq_length, # Use the hp value for max length per object\n",
        "      data_scale_factor=data_scale_factor, # Use the correct scale factor!\n",
        "      num_frames=400,        # Total frames for the whole animation (adjust as needed)\n",
        "      save_path=output_gif_path,\n",
        "      temperature=0.3,       # Adjust temperature for desired randomness\n",
        "      spacing_factor=1.3,    # Adjust spacing between objects\n",
        "      initial_offset=(0, 0)  # Start drawing at origin\n",
        "  )\n",
        "  print(\"\\nScene generation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_o-LvZAzWqG",
        "outputId": "608da033-210f-443f-b250-a074efe2b9d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting scene generation for classes: ['cat', 'apple', 'airplane']\n",
            "  Generating object 1/3: cat\n",
            "    Object cat added. Next offset: (115.1, 0.0)\n",
            "  Generating object 2/3: apple\n",
            "    Object apple added. Next offset: (308.8, 0.0)\n",
            "  Generating object 3/3: airplane\n",
            "    Object airplane added. Next offset: (663.4, 0.0)\n",
            "Creating vertically flipped scene animation 'my_cool_scene_animation_FLIPPED.gif' with 400 frames...\n",
            "Scene animation saved to my_cool_scene_animation_FLIPPED.gif\n",
            "\n",
            "Scene generation complete.\n"
          ]
        }
      ]
    }
  ]
}